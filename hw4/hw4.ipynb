{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習 HW4\n",
    "### 會計四 B06702064 林聖硯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (463715, 90)\n",
      "X_subtrain shape =  (417344, 90)\n",
      "X_valid shape =  (46371, 90)\n",
      "X_test shape =  (51630, 90)\n",
      "Y_train shape =  (463715,)\n",
      "Y_subtrain shape =  (417344,)\n",
      "Y_valid shape =  (46371,)\n",
      "Y_test shape =  (51630,)\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Load data\n",
    "with open('data/msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    #standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test'].astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_mean = Y_train.mean()\n",
    "Y_train_keep = Y_train.copy()\n",
    "Y_test_keep = Y_test.copy()\n",
    "Y_train = Y_train - y_mean\n",
    "Y_test = Y_test - y_mean\n",
    "\n",
    "\n",
    "#validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print(\"X_test shape = \", X_test.shape)\n",
    "print(\"Y_train shape = \", Y_train.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print(\"Y_test shape = \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, Y_train)\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 9.510160684544399\n",
      "First five weights= [ 5.30975265 -2.88088114 -1.53234348  0.05737583 -0.33952889]\n"
     ]
    }
   ],
   "source": [
    "print('RMSE = {}'.format(mean_squared_error(Y_test, y_pred, squared=False)))\n",
    "print('First five weights= {}'.format(reg.coef_[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  #Characterizes a dataset for PyTorch\n",
    "    def __init__(self, Xnp, Ynp):\n",
    "        #Initialization, passing Xnp and Ynp\n",
    "        self.labels = Ynp\n",
    "        self.nobs = Xnp.shape[0]        \n",
    "        self.Xnp = Xnp\n",
    "        self.Ynp = Ynp\n",
    "    def __len__(self):\n",
    "        #Denotes the total numㄖber of samples\n",
    "        return self.nobs\n",
    "    def __getitem__(self, index):\n",
    "        #Generates one sample of data      \n",
    "        X = self.Xnp[index]\n",
    "        y = self.Ynp[index]\n",
    "        return X, y\n",
    "     \n",
    "subtrainset = Dataset(X_subtrain, Y_subtrain)\n",
    "validset = Dataset(X_valid, Y_valid)\n",
    "trainset = Dataset(X_train, Y_train)   \n",
    "testset = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrainloader = data.DataLoader(subtrainset, batch_size=1000)\n",
    "validloader = data.DataLoader(validset, batch_size=1000)\n",
    "trainloader = data.DataLoader(trainset, batch_size=1000)\n",
    "testloader = data.DataLoader(testset, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myMLP():\n",
    "    def __init__(self):\n",
    "    self.batch_num = None\n",
    "    self.subtrainRMSE = list()\n",
    "    self.validRMSE = list()\n",
    "    self.mse = torch.nn.MSELoss(reduction = 'mean')\n",
    "    self.sse = torch.nn.MSELoss(reduction = 'sum')\n",
    "    self.net = None\n",
    "\n",
    "    def Net(self, input_shape, device, H = 45, dropout = False):\n",
    "        D_in = input_shape\n",
    "        D_out = 1\n",
    "        if dropout == False:\n",
    "            net = torch.nn.Sequential(\n",
    "              torch.nn.Linear(D_in, H),  \n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, D_out)\n",
    "          )\n",
    "\n",
    "        else:\n",
    "            net = torch.nn.Sequential(\n",
    "              torch.nn.Linear(D_in, H),  \n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Dropout(p = 0.5),\n",
    "              torch.nn.Linear(H, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Dropout(p = 0.5),\n",
    "              torch.nn.Linear(H, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Dropout(p = 0.5),\n",
    "              torch.nn.Linear(H, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Dropout(p = 0.5),\n",
    "              torch.nn.Linear(H, D_out)\n",
    "          )\n",
    "        # convert everything to float precision. \n",
    "        net = net.float()\n",
    "        # move the model to device (i.e., cpu or gpu)\n",
    "        net = net.to(device)\n",
    "        return net\n",
    "\n",
    "  # Train\n",
    "    def train(self, device, model, nepoch, log_interval, optimizer, train_loader, valid_loader, verbose = True):\n",
    "\n",
    "        # Early stopping\n",
    "        min_valid_loss = 10000000\n",
    "        best_step_count = 0\n",
    "        patience = 5000\n",
    "\n",
    "        sum_loss_MSE = 0\n",
    "        step_count = 0\n",
    "        for epoch_idx in range(1, nepoch+1):\n",
    "          #1 epoch = 400 batch (1 batch = 1000 data points)    \n",
    "          for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            #reshape target to two-dimensional array\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            step_count += 1        \n",
    "            model.train()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #loss for update        \n",
    "            loss_SSE = self.sse(outputs, targets)\n",
    "            #loss for show\n",
    "            loss_MSE = self.mse(outputs, targets)\n",
    "            loss_SSE.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss_MSE += loss_MSE\n",
    "            if step_count % log_interval == 0:\n",
    "              #1000 = batch_size, 100 = number of batch         \n",
    "                RMSE_train = (sum_loss_MSE / log_interval) ** (1/2)\n",
    "                RMSE_valid = self.validation(model, device, log_interval, valid_loader)\n",
    "                self.subtrainRMSE.append(RMSE_train)\n",
    "                self.validRMSE.append(RMSE_valid)\n",
    "                if verbose == True:\n",
    "                    print(\"Epoch %d Step %d training loss = %.3f validation loss = %3f (minibatch size = %d)\" % (epoch_idx, step_count, RMSE_train, RMSE_valid, len(targets))) \n",
    "\n",
    "                if RMSE_valid < min_valid_loss:\n",
    "                    best_step_count = step_count\n",
    "                    min_valid_loss = RMSE_valid\n",
    "                    best_model = model\n",
    "                sum_loss_MSE = 0 \n",
    "\n",
    "            if step_count >= best_step_count + patience and RMSE_valid > min_valid_loss:\n",
    "                if verbose == True:\n",
    "                    print('Early stopping!')\n",
    "                    print('='*50 + 'validation result' + '='*50)\n",
    "                    print('the best step is {} with minimum validation error = {}'.format(best_step_count, min_valid_loss))\n",
    "                self.batch_num = step_count\n",
    "                return best_model\n",
    "        if verbose == True:\n",
    "            print('='*50 + 'result' + '='*50)\n",
    "            print('the best step is {} with minimum validation error = {}'.format(best_step_count, min_valid_loss))\n",
    "        self.batch_num = step_count\n",
    "        return best_model\n",
    "\n",
    "\n",
    "    def validation(self, model, device, log_interval, valid_loader):\n",
    "    #切到evaluate的模式，不會進行gradient descent\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "      #1 epoch = 40 batch\n",
    "            for batch_idx, (inputs, targets) in enumerate(valid_loader):            \n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.reshape((-1, 1))\n",
    "                outputs = model(inputs)        \n",
    "                valid_loss = self.mse(outputs, targets)\n",
    "                total_loss += valid_loss\n",
    "        RMSE = (total_loss / batch_idx) ** (1/2)\n",
    "        return RMSE\n",
    "\n",
    "    def test(self, device, H, net, test_loader):\n",
    "        net.eval()\n",
    "        sum_test_MSE = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(test_loader):            \n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.reshape((-1, 1))\n",
    "                outputs = net(inputs)        \n",
    "                test_loss = self.mse(outputs, targets).item()\n",
    "                sum_test_MSE += test_loss \n",
    "        test_RMSE = (sum_test_MSE / batch_idx) ** (1/2)\n",
    "        return test_RMSE\n",
    "  \n",
    "    def plot(self):\n",
    "        #print subtraining result\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        x = np.linspace(100, self.batch_num, self.batch_num // 100)\n",
    "        ax.plot(x, self.subtrainRMSE, label = 'subtrain RMSE')\n",
    "        ax.plot(x, self.validRMSE, label = 'validation RMSE')\n",
    "        ax.legend()\n",
    "        #plt.title('H = {}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 75368,
     "status": "ok",
     "timestamp": 1608710827660,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "0y5FufovzwFB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a8196d7d-d370-4824-d1c7-ea721a73cdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 100 training loss = 10.840 validation loss = 9.860864 (minibatch size = 1000)\n",
      "Epoch 1 Step 200 training loss = 9.238 validation loss = 9.094234 (minibatch size = 1000)\n",
      "Epoch 1 Step 300 training loss = 9.066 validation loss = 8.953245 (minibatch size = 1000)\n",
      "Epoch 1 Step 400 training loss = 8.899 validation loss = 9.035228 (minibatch size = 1000)\n",
      "Epoch 2 Step 500 training loss = 8.918 validation loss = 8.878045 (minibatch size = 1000)\n",
      "Epoch 2 Step 600 training loss = 8.820 validation loss = 8.804682 (minibatch size = 1000)\n",
      "Epoch 2 Step 700 training loss = 8.835 validation loss = 8.834981 (minibatch size = 1000)\n",
      "Epoch 2 Step 800 training loss = 8.806 validation loss = 8.772099 (minibatch size = 1000)\n",
      "Epoch 3 Step 900 training loss = 8.742 validation loss = 8.761466 (minibatch size = 1000)\n",
      "Epoch 3 Step 1000 training loss = 8.767 validation loss = 8.798289 (minibatch size = 1000)\n",
      "Epoch 3 Step 1100 training loss = 8.713 validation loss = 8.754601 (minibatch size = 1000)\n",
      "Epoch 3 Step 1200 training loss = 8.753 validation loss = 9.166614 (minibatch size = 1000)\n",
      "Epoch 4 Step 1300 training loss = 8.683 validation loss = 8.773139 (minibatch size = 1000)\n",
      "Epoch 4 Step 1400 training loss = 8.690 validation loss = 8.783874 (minibatch size = 1000)\n",
      "Epoch 4 Step 1500 training loss = 8.661 validation loss = 8.686892 (minibatch size = 1000)\n",
      "Epoch 4 Step 1600 training loss = 8.718 validation loss = 8.772392 (minibatch size = 1000)\n",
      "Epoch 5 Step 1700 training loss = 8.646 validation loss = 8.738353 (minibatch size = 1000)\n",
      "Epoch 5 Step 1800 training loss = 8.640 validation loss = 8.749910 (minibatch size = 1000)\n",
      "Epoch 5 Step 1900 training loss = 8.597 validation loss = 8.686469 (minibatch size = 1000)\n",
      "Epoch 5 Step 2000 training loss = 8.680 validation loss = 8.708433 (minibatch size = 1000)\n",
      "Epoch 6 Step 2100 training loss = 8.621 validation loss = 8.708744 (minibatch size = 1000)\n",
      "Epoch 6 Step 2200 training loss = 8.632 validation loss = 8.779897 (minibatch size = 1000)\n",
      "Epoch 6 Step 2300 training loss = 8.559 validation loss = 8.710643 (minibatch size = 1000)\n",
      "Epoch 6 Step 2400 training loss = 8.624 validation loss = 8.722429 (minibatch size = 1000)\n",
      "Epoch 6 Step 2500 training loss = 8.579 validation loss = 8.752927 (minibatch size = 1000)\n",
      "Epoch 7 Step 2600 training loss = 8.623 validation loss = 8.717588 (minibatch size = 1000)\n",
      "Epoch 7 Step 2700 training loss = 8.539 validation loss = 8.757536 (minibatch size = 1000)\n",
      "Epoch 7 Step 2800 training loss = 8.608 validation loss = 8.684928 (minibatch size = 1000)\n",
      "Epoch 7 Step 2900 training loss = 8.560 validation loss = 8.680108 (minibatch size = 1000)\n",
      "Epoch 8 Step 3000 training loss = 8.566 validation loss = 8.721653 (minibatch size = 1000)\n",
      "Epoch 8 Step 3100 training loss = 8.532 validation loss = 8.682661 (minibatch size = 1000)\n",
      "Epoch 8 Step 3200 training loss = 8.571 validation loss = 8.675797 (minibatch size = 1000)\n",
      "Epoch 8 Step 3300 training loss = 8.576 validation loss = 8.716368 (minibatch size = 1000)\n",
      "Epoch 9 Step 3400 training loss = 8.517 validation loss = 8.716438 (minibatch size = 1000)\n",
      "Epoch 9 Step 3500 training loss = 8.554 validation loss = 8.659537 (minibatch size = 1000)\n",
      "Epoch 9 Step 3600 training loss = 8.502 validation loss = 8.682186 (minibatch size = 1000)\n",
      "Epoch 9 Step 3700 training loss = 8.575 validation loss = 8.739450 (minibatch size = 1000)\n",
      "Epoch 10 Step 3800 training loss = 8.523 validation loss = 8.694526 (minibatch size = 1000)\n",
      "Epoch 10 Step 3900 training loss = 8.520 validation loss = 8.687060 (minibatch size = 1000)\n",
      "Epoch 10 Step 4000 training loss = 8.480 validation loss = 8.733657 (minibatch size = 1000)\n",
      "Epoch 10 Step 4100 training loss = 8.567 validation loss = 8.735892 (minibatch size = 1000)\n",
      "Epoch 11 Step 4200 training loss = 8.508 validation loss = 8.838754 (minibatch size = 1000)\n",
      "Epoch 11 Step 4300 training loss = 8.514 validation loss = 8.706798 (minibatch size = 1000)\n",
      "Epoch 11 Step 4400 training loss = 8.461 validation loss = 8.733996 (minibatch size = 1000)\n",
      "Epoch 11 Step 4500 training loss = 8.552 validation loss = 8.684005 (minibatch size = 1000)\n",
      "Epoch 12 Step 4600 training loss = 8.478 validation loss = 8.679962 (minibatch size = 1000)\n",
      "Epoch 12 Step 4700 training loss = 8.517 validation loss = 8.689131 (minibatch size = 1000)\n",
      "Epoch 12 Step 4800 training loss = 8.420 validation loss = 8.671093 (minibatch size = 1000)\n",
      "Epoch 12 Step 4900 training loss = 8.540 validation loss = 8.746778 (minibatch size = 1000)\n",
      "Epoch 12 Step 5000 training loss = 8.456 validation loss = 8.729655 (minibatch size = 1000)\n",
      "Epoch 13 Step 5100 training loss = 8.508 validation loss = 8.751602 (minibatch size = 1000)\n",
      "Epoch 13 Step 5200 training loss = 8.433 validation loss = 8.706089 (minibatch size = 1000)\n",
      "Epoch 13 Step 5300 training loss = 8.510 validation loss = 8.685806 (minibatch size = 1000)\n",
      "Epoch 13 Step 5400 training loss = 8.479 validation loss = 8.735534 (minibatch size = 1000)\n",
      "Epoch 14 Step 5500 training loss = 8.442 validation loss = 8.690063 (minibatch size = 1000)\n",
      "Epoch 14 Step 5600 training loss = 8.472 validation loss = 8.681815 (minibatch size = 1000)\n",
      "Epoch 14 Step 5700 training loss = 8.461 validation loss = 8.715055 (minibatch size = 1000)\n",
      "Epoch 14 Step 5800 training loss = 8.475 validation loss = 8.761999 (minibatch size = 1000)\n",
      "Epoch 15 Step 5900 training loss = 8.460 validation loss = 8.698034 (minibatch size = 1000)\n",
      "Epoch 15 Step 6000 training loss = 8.451 validation loss = 8.689392 (minibatch size = 1000)\n",
      "Epoch 15 Step 6100 training loss = 8.426 validation loss = 8.719736 (minibatch size = 1000)\n",
      "Epoch 15 Step 6200 training loss = 8.501 validation loss = 8.686753 (minibatch size = 1000)\n",
      "Epoch 16 Step 6300 training loss = 8.442 validation loss = 8.697331 (minibatch size = 1000)\n",
      "Epoch 16 Step 6400 training loss = 8.435 validation loss = 8.709449 (minibatch size = 1000)\n",
      "Epoch 16 Step 6500 training loss = 8.412 validation loss = 8.896916 (minibatch size = 1000)\n",
      "Epoch 16 Step 6600 training loss = 8.484 validation loss = 8.782329 (minibatch size = 1000)\n",
      "Epoch 17 Step 6700 training loss = 8.432 validation loss = 8.732515 (minibatch size = 1000)\n",
      "Epoch 17 Step 6800 training loss = 8.454 validation loss = 8.686784 (minibatch size = 1000)\n",
      "Epoch 17 Step 6900 training loss = 8.379 validation loss = 8.685161 (minibatch size = 1000)\n",
      "Epoch 17 Step 7000 training loss = 8.468 validation loss = 8.704151 (minibatch size = 1000)\n",
      "Epoch 17 Step 7100 training loss = 8.411 validation loss = 8.776712 (minibatch size = 1000)\n",
      "Epoch 18 Step 7200 training loss = 8.449 validation loss = 8.705949 (minibatch size = 1000)\n",
      "Epoch 18 Step 7300 training loss = 8.368 validation loss = 8.710105 (minibatch size = 1000)\n",
      "Epoch 18 Step 7400 training loss = 8.469 validation loss = 8.675838 (minibatch size = 1000)\n",
      "Epoch 18 Step 7500 training loss = 8.403 validation loss = 8.681072 (minibatch size = 1000)\n",
      "Epoch 19 Step 7600 training loss = 8.415 validation loss = 8.729931 (minibatch size = 1000)\n",
      "Epoch 19 Step 7700 training loss = 8.386 validation loss = 8.681466 (minibatch size = 1000)\n",
      "Epoch 19 Step 7800 training loss = 8.436 validation loss = 8.737357 (minibatch size = 1000)\n",
      "Epoch 19 Step 7900 training loss = 8.430 validation loss = 8.752324 (minibatch size = 1000)\n",
      "Epoch 20 Step 8000 training loss = 8.383 validation loss = 8.716528 (minibatch size = 1000)\n",
      "Epoch 20 Step 8100 training loss = 8.419 validation loss = 8.716766 (minibatch size = 1000)\n",
      "Epoch 20 Step 8200 training loss = 8.367 validation loss = 8.697859 (minibatch size = 1000)\n",
      "Epoch 20 Step 8300 training loss = 8.452 validation loss = 8.771935 (minibatch size = 1000)\n",
      "Epoch 21 Step 8400 training loss = 8.388 validation loss = 8.680557 (minibatch size = 1000)\n",
      "Epoch 21 Step 8500 training loss = 8.404 validation loss = 8.710950 (minibatch size = 1000)\n",
      "Early stopping!\n",
      "==================================================validation result==================================================\n",
      "the best step is 3500 with minimum validation error = 8.659537315368652\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H = 45\n",
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "Q2_model = myMLP()\n",
    "net = Q2_model.Net(input_shape, device, H)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "model = Q2_model.train(device, net, nepoch, log_interval, optimizer, subtrainloader, validloader, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1430,
     "status": "ok",
     "timestamp": 1608710837349,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "LBA54EkWgUqa",
    "outputId": "0fdece8c-6c42-4efd-f803-dc177eeba8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 45, test_RMSE = 8.99610570623732\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_RMSE = Q2_model.test(device, H, model, testloader)\n",
    "print('when H = {}, test_RMSE = {}'.format(H, test_RMSE))\n",
    "Q2_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從圖形中可以看出，training RMSE持續下降但validation RMSE則到一個階段就停止，顯示出模型可能有overfitting的問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vETM5QsDmR_o"
   },
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR_hXh0ImSoC",
    "outputId": "fec888f0-7ac3-4638-c250-1c02c51db3da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 45, test_RMSE = 8.998106697083783\n",
      "when H = 90, test_RMSE = 9.142651040361425\n",
      "when H = 180, test_RMSE = 9.551088753182057\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H_list = [45, 90, 180]\n",
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "for H in H_list: \n",
    "    Q3_model = myMLP()\n",
    "    net = Q3_model.Net(input_shape, device, H)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    model = Q3_model.train(device, net, nepoch, log_interval, optimizer, subtrainloader, validloader, verbose = False)\n",
    "    test_RMSE = Q3_model.test(device, H, model, testloader)\n",
    "    print('when H = {}, test_RMSE = {}'.format(H, test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從結果可以看出，當H(hidden nodes)越大時，模型overfit的會越嚴重\n",
    "雖然沒有把圖顯示在這裡(怕占版面刪除了)，但是觀察H=45, 90, 180的圖可以發現subtraining error和validation error最後的差距隨著H上升會越來越大(即使有early stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTwZ57-Y8fIw"
   },
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 736198,
     "status": "ok",
     "timestamp": 1608712599887,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "GHrBVKMgmxAj",
    "outputId": "cc2bb3b2-b65b-4536-dea8-985990a181f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 45, weight decay = 0.1, test_RMSE = 8.961193385537408\n",
      "when H = 45, weight decay = 0.2, test_RMSE = 8.982916057015082\n",
      "when H = 45, weight decay = 0.4, test_RMSE = 9.078998487217396\n",
      "when H = 90, weight decay = 0.1, test_RMSE = 9.158906479934902\n",
      "when H = 90, weight decay = 0.2, test_RMSE = 9.1348187872297\n",
      "when H = 90, weight decay = 0.4, test_RMSE = 9.179390672349518\n",
      "when H = 180, weight decay = 0.1, test_RMSE = 9.52585461310313\n",
      "when H = 180, weight decay = 0.2, test_RMSE = 9.300641460844524\n",
      "when H = 180, weight decay = 0.4, test_RMSE = 9.477085527345936\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H_list = [45, 90, 180]\n",
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "#parameters\n",
    "H_list = [45, 90, 180]\n",
    "weight_list = [0.1, 0.2, 0.4]\n",
    "\n",
    "#model \n",
    "for H in H_list:\n",
    "    for weight in weight_list:\n",
    "        Q4_model = myMLP()\n",
    "        net = Q4_model.Net(input_shape, device, H)\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr = lr, weight_decay = weight)\n",
    "        model = Q4_model.train(device, net, nepoch, log_interval, optimizer, subtrainloader, validloader, verbose = False)\n",
    "        test_RMSE = Q4_model.test(device, H, model, testloader)\n",
    "        print('when H = {}, weight decay = {}, test_RMSE = {}'.format(H, weight, test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如同前面的結果可以發現H越上升, overfitting的問題越嚴重；且不是在所有情況下，weight decay上升，test RMSE會下降，weight decay不一定能有效減少overfitting的問題。\n",
    "在此情況下H應該選擇45為最佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTPqTg5s-9oS"
   },
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 465495,
     "status": "ok",
     "timestamp": 1608734537362,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "knJ4SwRDoYYz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c1a1e0dc-b3de-4e05-c2d6-478ad8cd4e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 100 training loss = 10.502 validation loss = 9.562896 (minibatch size = 1000)\n",
      "Epoch 1 Step 200 training loss = 9.539 validation loss = 9.208777 (minibatch size = 1000)\n",
      "Epoch 1 Step 300 training loss = 9.450 validation loss = 9.078730 (minibatch size = 1000)\n",
      "Epoch 1 Step 400 training loss = 9.258 validation loss = 9.013576 (minibatch size = 1000)\n",
      "Epoch 2 Step 500 training loss = 9.255 validation loss = 8.969096 (minibatch size = 1000)\n",
      "Epoch 2 Step 600 training loss = 9.183 validation loss = 9.003074 (minibatch size = 1000)\n",
      "Epoch 2 Step 700 training loss = 9.178 validation loss = 8.941461 (minibatch size = 1000)\n",
      "Epoch 2 Step 800 training loss = 9.141 validation loss = 8.936812 (minibatch size = 1000)\n",
      "Epoch 3 Step 900 training loss = 9.084 validation loss = 8.897058 (minibatch size = 1000)\n",
      "Epoch 3 Step 1000 training loss = 9.131 validation loss = 8.866661 (minibatch size = 1000)\n",
      "Epoch 3 Step 1100 training loss = 9.076 validation loss = 8.875486 (minibatch size = 1000)\n",
      "Epoch 3 Step 1200 training loss = 9.107 validation loss = 8.851742 (minibatch size = 1000)\n",
      "Epoch 4 Step 1300 training loss = 9.035 validation loss = 8.846981 (minibatch size = 1000)\n",
      "Epoch 4 Step 1400 training loss = 9.059 validation loss = 8.861193 (minibatch size = 1000)\n",
      "Epoch 4 Step 1500 training loss = 9.034 validation loss = 8.843407 (minibatch size = 1000)\n",
      "Epoch 4 Step 1600 training loss = 9.081 validation loss = 8.832997 (minibatch size = 1000)\n",
      "Epoch 5 Step 1700 training loss = 9.015 validation loss = 8.822498 (minibatch size = 1000)\n",
      "Epoch 5 Step 1800 training loss = 9.023 validation loss = 8.832170 (minibatch size = 1000)\n",
      "Epoch 5 Step 1900 training loss = 8.999 validation loss = 8.806320 (minibatch size = 1000)\n",
      "Epoch 5 Step 2000 training loss = 9.029 validation loss = 8.842943 (minibatch size = 1000)\n",
      "Epoch 6 Step 2100 training loss = 9.007 validation loss = 8.806056 (minibatch size = 1000)\n",
      "Epoch 6 Step 2200 training loss = 8.999 validation loss = 8.798477 (minibatch size = 1000)\n",
      "Epoch 6 Step 2300 training loss = 8.957 validation loss = 8.811593 (minibatch size = 1000)\n",
      "Epoch 6 Step 2400 training loss = 8.998 validation loss = 8.800343 (minibatch size = 1000)\n",
      "Epoch 6 Step 2500 training loss = 8.963 validation loss = 8.793900 (minibatch size = 1000)\n",
      "Epoch 7 Step 2600 training loss = 8.997 validation loss = 8.792242 (minibatch size = 1000)\n",
      "Epoch 7 Step 2700 training loss = 8.947 validation loss = 8.776961 (minibatch size = 1000)\n",
      "Epoch 7 Step 2800 training loss = 8.982 validation loss = 8.777233 (minibatch size = 1000)\n",
      "Epoch 7 Step 2900 training loss = 8.955 validation loss = 8.788990 (minibatch size = 1000)\n",
      "Epoch 8 Step 3000 training loss = 8.943 validation loss = 8.768208 (minibatch size = 1000)\n",
      "Epoch 8 Step 3100 training loss = 8.932 validation loss = 8.768847 (minibatch size = 1000)\n",
      "Epoch 8 Step 3200 training loss = 8.955 validation loss = 8.784674 (minibatch size = 1000)\n",
      "Epoch 8 Step 3300 training loss = 8.981 validation loss = 8.770144 (minibatch size = 1000)\n",
      "Epoch 9 Step 3400 training loss = 8.916 validation loss = 8.759632 (minibatch size = 1000)\n",
      "Epoch 9 Step 3500 training loss = 8.946 validation loss = 8.785929 (minibatch size = 1000)\n",
      "Epoch 9 Step 3600 training loss = 8.910 validation loss = 8.758379 (minibatch size = 1000)\n",
      "Epoch 9 Step 3700 training loss = 8.968 validation loss = 8.740650 (minibatch size = 1000)\n",
      "Epoch 10 Step 3800 training loss = 8.907 validation loss = 8.754068 (minibatch size = 1000)\n",
      "Epoch 10 Step 3900 training loss = 8.933 validation loss = 8.755276 (minibatch size = 1000)\n",
      "Epoch 10 Step 4000 training loss = 8.889 validation loss = 8.738205 (minibatch size = 1000)\n",
      "Epoch 10 Step 4100 training loss = 8.989 validation loss = 8.756380 (minibatch size = 1000)\n",
      "Epoch 11 Step 4200 training loss = 8.907 validation loss = 8.764713 (minibatch size = 1000)\n",
      "Epoch 11 Step 4300 training loss = 8.907 validation loss = 8.744479 (minibatch size = 1000)\n",
      "Epoch 11 Step 4400 training loss = 8.900 validation loss = 8.732710 (minibatch size = 1000)\n",
      "Epoch 11 Step 4500 training loss = 8.951 validation loss = 8.771322 (minibatch size = 1000)\n",
      "Epoch 12 Step 4600 training loss = 8.901 validation loss = 8.742120 (minibatch size = 1000)\n",
      "Epoch 12 Step 4700 training loss = 8.910 validation loss = 8.749832 (minibatch size = 1000)\n",
      "Epoch 12 Step 4800 training loss = 8.864 validation loss = 8.760774 (minibatch size = 1000)\n",
      "Epoch 12 Step 4900 training loss = 8.943 validation loss = 8.740232 (minibatch size = 1000)\n",
      "Epoch 12 Step 5000 training loss = 8.891 validation loss = 8.743816 (minibatch size = 1000)\n",
      "Epoch 13 Step 5100 training loss = 8.929 validation loss = 8.733539 (minibatch size = 1000)\n",
      "Epoch 13 Step 5200 training loss = 8.855 validation loss = 8.763036 (minibatch size = 1000)\n",
      "Epoch 13 Step 5300 training loss = 8.925 validation loss = 8.759866 (minibatch size = 1000)\n",
      "Epoch 13 Step 5400 training loss = 8.896 validation loss = 8.747195 (minibatch size = 1000)\n",
      "Epoch 14 Step 5500 training loss = 8.876 validation loss = 8.735144 (minibatch size = 1000)\n",
      "Epoch 14 Step 5600 training loss = 8.918 validation loss = 8.730601 (minibatch size = 1000)\n",
      "Epoch 14 Step 5700 training loss = 8.888 validation loss = 8.740057 (minibatch size = 1000)\n",
      "Epoch 14 Step 5800 training loss = 8.906 validation loss = 8.733040 (minibatch size = 1000)\n",
      "Epoch 15 Step 5900 training loss = 8.869 validation loss = 8.743579 (minibatch size = 1000)\n",
      "Epoch 15 Step 6000 training loss = 8.882 validation loss = 8.734872 (minibatch size = 1000)\n",
      "Epoch 15 Step 6100 training loss = 8.873 validation loss = 8.727662 (minibatch size = 1000)\n",
      "Epoch 15 Step 6200 training loss = 8.912 validation loss = 8.727677 (minibatch size = 1000)\n",
      "Epoch 16 Step 6300 training loss = 8.858 validation loss = 8.732789 (minibatch size = 1000)\n",
      "Epoch 16 Step 6400 training loss = 8.868 validation loss = 8.739351 (minibatch size = 1000)\n",
      "Epoch 16 Step 6500 training loss = 8.863 validation loss = 8.727394 (minibatch size = 1000)\n",
      "Epoch 16 Step 6600 training loss = 8.899 validation loss = 8.745506 (minibatch size = 1000)\n",
      "Epoch 17 Step 6700 training loss = 8.878 validation loss = 8.735124 (minibatch size = 1000)\n",
      "Epoch 17 Step 6800 training loss = 8.880 validation loss = 8.724064 (minibatch size = 1000)\n",
      "Epoch 17 Step 6900 training loss = 8.839 validation loss = 8.728257 (minibatch size = 1000)\n",
      "Epoch 17 Step 7000 training loss = 8.892 validation loss = 8.729365 (minibatch size = 1000)\n",
      "Epoch 17 Step 7100 training loss = 8.848 validation loss = 8.745986 (minibatch size = 1000)\n",
      "Epoch 18 Step 7200 training loss = 8.893 validation loss = 8.733068 (minibatch size = 1000)\n",
      "Epoch 18 Step 7300 training loss = 8.829 validation loss = 8.723069 (minibatch size = 1000)\n",
      "Epoch 18 Step 7400 training loss = 8.893 validation loss = 8.708738 (minibatch size = 1000)\n",
      "Epoch 18 Step 7500 training loss = 8.870 validation loss = 8.732880 (minibatch size = 1000)\n",
      "Epoch 19 Step 7600 training loss = 8.855 validation loss = 8.730589 (minibatch size = 1000)\n",
      "Epoch 19 Step 7700 training loss = 8.836 validation loss = 8.723825 (minibatch size = 1000)\n",
      "Epoch 19 Step 7800 training loss = 8.868 validation loss = 8.733448 (minibatch size = 1000)\n",
      "Epoch 19 Step 7900 training loss = 8.883 validation loss = 8.719609 (minibatch size = 1000)\n",
      "Epoch 20 Step 8000 training loss = 8.809 validation loss = 8.720288 (minibatch size = 1000)\n",
      "Epoch 20 Step 8100 training loss = 8.875 validation loss = 8.725865 (minibatch size = 1000)\n",
      "Epoch 20 Step 8200 training loss = 8.828 validation loss = 8.715719 (minibatch size = 1000)\n",
      "Epoch 20 Step 8300 training loss = 8.897 validation loss = 8.693149 (minibatch size = 1000)\n",
      "Epoch 21 Step 8400 training loss = 8.821 validation loss = 8.698280 (minibatch size = 1000)\n",
      "Epoch 21 Step 8500 training loss = 8.868 validation loss = 8.726356 (minibatch size = 1000)\n",
      "Epoch 21 Step 8600 training loss = 8.829 validation loss = 8.710156 (minibatch size = 1000)\n",
      "Epoch 21 Step 8700 training loss = 8.880 validation loss = 8.718548 (minibatch size = 1000)\n",
      "Epoch 22 Step 8800 training loss = 8.838 validation loss = 8.718599 (minibatch size = 1000)\n",
      "Epoch 22 Step 8900 training loss = 8.845 validation loss = 8.707334 (minibatch size = 1000)\n",
      "Epoch 22 Step 9000 training loss = 8.823 validation loss = 8.719153 (minibatch size = 1000)\n",
      "Epoch 22 Step 9100 training loss = 8.884 validation loss = 8.725998 (minibatch size = 1000)\n",
      "Epoch 23 Step 9200 training loss = 8.853 validation loss = 8.703017 (minibatch size = 1000)\n",
      "Epoch 23 Step 9300 training loss = 8.867 validation loss = 8.724162 (minibatch size = 1000)\n",
      "Epoch 23 Step 9400 training loss = 8.794 validation loss = 8.721898 (minibatch size = 1000)\n",
      "Epoch 23 Step 9500 training loss = 8.869 validation loss = 8.716498 (minibatch size = 1000)\n",
      "Epoch 23 Step 9600 training loss = 8.835 validation loss = 8.711075 (minibatch size = 1000)\n",
      "Epoch 24 Step 9700 training loss = 8.868 validation loss = 8.706536 (minibatch size = 1000)\n",
      "Epoch 24 Step 9800 training loss = 8.813 validation loss = 8.724305 (minibatch size = 1000)\n",
      "Epoch 24 Step 9900 training loss = 8.864 validation loss = 8.735119 (minibatch size = 1000)\n",
      "Epoch 24 Step 10000 training loss = 8.847 validation loss = 8.711270 (minibatch size = 1000)\n",
      "Epoch 25 Step 10100 training loss = 8.808 validation loss = 8.712613 (minibatch size = 1000)\n",
      "Epoch 25 Step 10200 training loss = 8.838 validation loss = 8.696727 (minibatch size = 1000)\n",
      "Epoch 25 Step 10300 training loss = 8.845 validation loss = 8.716784 (minibatch size = 1000)\n",
      "Epoch 25 Step 10400 training loss = 8.850 validation loss = 8.703024 (minibatch size = 1000)\n",
      "Epoch 26 Step 10500 training loss = 8.815 validation loss = 8.703485 (minibatch size = 1000)\n",
      "Epoch 26 Step 10600 training loss = 8.839 validation loss = 8.719661 (minibatch size = 1000)\n",
      "Epoch 26 Step 10700 training loss = 8.809 validation loss = 8.704302 (minibatch size = 1000)\n",
      "Epoch 26 Step 10800 training loss = 8.875 validation loss = 8.699996 (minibatch size = 1000)\n",
      "Epoch 27 Step 10900 training loss = 8.830 validation loss = 8.708217 (minibatch size = 1000)\n",
      "Epoch 27 Step 11000 training loss = 8.828 validation loss = 8.707419 (minibatch size = 1000)\n",
      "Epoch 27 Step 11100 training loss = 8.796 validation loss = 8.696345 (minibatch size = 1000)\n",
      "Epoch 27 Step 11200 training loss = 8.871 validation loss = 8.716756 (minibatch size = 1000)\n",
      "Epoch 28 Step 11300 training loss = 8.832 validation loss = 8.709572 (minibatch size = 1000)\n",
      "Epoch 28 Step 11400 training loss = 8.842 validation loss = 8.706347 (minibatch size = 1000)\n",
      "Epoch 28 Step 11500 training loss = 8.791 validation loss = 8.698731 (minibatch size = 1000)\n",
      "Epoch 28 Step 11600 training loss = 8.839 validation loss = 8.699970 (minibatch size = 1000)\n",
      "Epoch 28 Step 11700 training loss = 8.834 validation loss = 8.715007 (minibatch size = 1000)\n",
      "Epoch 29 Step 11800 training loss = 8.861 validation loss = 8.703552 (minibatch size = 1000)\n",
      "Epoch 29 Step 11900 training loss = 8.779 validation loss = 8.704711 (minibatch size = 1000)\n",
      "Epoch 29 Step 12000 training loss = 8.872 validation loss = 8.694934 (minibatch size = 1000)\n",
      "Epoch 29 Step 12100 training loss = 8.809 validation loss = 8.705659 (minibatch size = 1000)\n",
      "Epoch 30 Step 12200 training loss = 8.828 validation loss = 8.720266 (minibatch size = 1000)\n",
      "Epoch 30 Step 12300 training loss = 8.794 validation loss = 8.714968 (minibatch size = 1000)\n",
      "Epoch 30 Step 12400 training loss = 8.848 validation loss = 8.710019 (minibatch size = 1000)\n",
      "Epoch 30 Step 12500 training loss = 8.845 validation loss = 8.704726 (minibatch size = 1000)\n",
      "Epoch 31 Step 12600 training loss = 8.786 validation loss = 8.691549 (minibatch size = 1000)\n",
      "Epoch 31 Step 12700 training loss = 8.840 validation loss = 8.705618 (minibatch size = 1000)\n",
      "Epoch 31 Step 12800 training loss = 8.786 validation loss = 8.691449 (minibatch size = 1000)\n",
      "Epoch 31 Step 12900 training loss = 8.859 validation loss = 8.687386 (minibatch size = 1000)\n",
      "Epoch 32 Step 13000 training loss = 8.811 validation loss = 8.697156 (minibatch size = 1000)\n",
      "Epoch 32 Step 13100 training loss = 8.819 validation loss = 8.705625 (minibatch size = 1000)\n",
      "Epoch 32 Step 13200 training loss = 8.788 validation loss = 8.688639 (minibatch size = 1000)\n",
      "Epoch 32 Step 13300 training loss = 8.867 validation loss = 8.717969 (minibatch size = 1000)\n",
      "Epoch 33 Step 13400 training loss = 8.808 validation loss = 8.705688 (minibatch size = 1000)\n",
      "Epoch 33 Step 13500 training loss = 8.832 validation loss = 8.692888 (minibatch size = 1000)\n",
      "Epoch 33 Step 13600 training loss = 8.783 validation loss = 8.700525 (minibatch size = 1000)\n",
      "Epoch 33 Step 13700 training loss = 8.836 validation loss = 8.718699 (minibatch size = 1000)\n",
      "Epoch 34 Step 13800 training loss = 8.841 validation loss = 8.687046 (minibatch size = 1000)\n",
      "Epoch 34 Step 13900 training loss = 8.810 validation loss = 8.695089 (minibatch size = 1000)\n",
      "Epoch 34 Step 14000 training loss = 8.775 validation loss = 8.714168 (minibatch size = 1000)\n",
      "Epoch 34 Step 14100 training loss = 8.838 validation loss = 8.688116 (minibatch size = 1000)\n",
      "Epoch 34 Step 14200 training loss = 8.814 validation loss = 8.697236 (minibatch size = 1000)\n",
      "Epoch 35 Step 14300 training loss = 8.841 validation loss = 8.682415 (minibatch size = 1000)\n",
      "Epoch 35 Step 14400 training loss = 8.777 validation loss = 8.699205 (minibatch size = 1000)\n",
      "Epoch 35 Step 14500 training loss = 8.842 validation loss = 8.703527 (minibatch size = 1000)\n",
      "Epoch 35 Step 14600 training loss = 8.812 validation loss = 8.702837 (minibatch size = 1000)\n",
      "Epoch 36 Step 14700 training loss = 8.795 validation loss = 8.690087 (minibatch size = 1000)\n",
      "Epoch 36 Step 14800 training loss = 8.802 validation loss = 8.688469 (minibatch size = 1000)\n",
      "Epoch 36 Step 14900 training loss = 8.808 validation loss = 8.702851 (minibatch size = 1000)\n",
      "Epoch 36 Step 15000 training loss = 8.845 validation loss = 8.689942 (minibatch size = 1000)\n",
      "Epoch 37 Step 15100 training loss = 8.799 validation loss = 8.690924 (minibatch size = 1000)\n",
      "Epoch 37 Step 15200 training loss = 8.828 validation loss = 8.711889 (minibatch size = 1000)\n",
      "Epoch 37 Step 15300 training loss = 8.804 validation loss = 8.690354 (minibatch size = 1000)\n",
      "Epoch 37 Step 15400 training loss = 8.860 validation loss = 8.688772 (minibatch size = 1000)\n",
      "Epoch 38 Step 15500 training loss = 8.813 validation loss = 8.689553 (minibatch size = 1000)\n",
      "Epoch 38 Step 15600 training loss = 8.808 validation loss = 8.694568 (minibatch size = 1000)\n",
      "Epoch 38 Step 15700 training loss = 8.769 validation loss = 8.693934 (minibatch size = 1000)\n",
      "Epoch 38 Step 15800 training loss = 8.849 validation loss = 8.709902 (minibatch size = 1000)\n",
      "Epoch 39 Step 15900 training loss = 8.818 validation loss = 8.688608 (minibatch size = 1000)\n",
      "Epoch 39 Step 16000 training loss = 8.824 validation loss = 8.694070 (minibatch size = 1000)\n",
      "Epoch 39 Step 16100 training loss = 8.769 validation loss = 8.687184 (minibatch size = 1000)\n",
      "Epoch 39 Step 16200 training loss = 8.842 validation loss = 8.706719 (minibatch size = 1000)\n",
      "Epoch 39 Step 16300 training loss = 8.787 validation loss = 8.699944 (minibatch size = 1000)\n",
      "Epoch 40 Step 16400 training loss = 8.822 validation loss = 8.694057 (minibatch size = 1000)\n",
      "Epoch 40 Step 16500 training loss = 8.749 validation loss = 8.689121 (minibatch size = 1000)\n",
      "Epoch 40 Step 16600 training loss = 8.855 validation loss = 8.688925 (minibatch size = 1000)\n",
      "Epoch 40 Step 16700 training loss = 8.784 validation loss = 8.697183 (minibatch size = 1000)\n",
      "Epoch 41 Step 16800 training loss = 8.821 validation loss = 8.710420 (minibatch size = 1000)\n",
      "Epoch 41 Step 16900 training loss = 8.789 validation loss = 8.715639 (minibatch size = 1000)\n",
      "Epoch 41 Step 17000 training loss = 8.813 validation loss = 8.695945 (minibatch size = 1000)\n",
      "Epoch 41 Step 17100 training loss = 8.843 validation loss = 8.691132 (minibatch size = 1000)\n",
      "Epoch 42 Step 17200 training loss = 8.774 validation loss = 8.694699 (minibatch size = 1000)\n",
      "Epoch 42 Step 17300 training loss = 8.819 validation loss = 8.688421 (minibatch size = 1000)\n",
      "Epoch 42 Step 17400 training loss = 8.755 validation loss = 8.679938 (minibatch size = 1000)\n",
      "Epoch 42 Step 17500 training loss = 8.836 validation loss = 8.674814 (minibatch size = 1000)\n",
      "Epoch 43 Step 17600 training loss = 8.805 validation loss = 8.683578 (minibatch size = 1000)\n",
      "Epoch 43 Step 17700 training loss = 8.812 validation loss = 8.693276 (minibatch size = 1000)\n",
      "Epoch 43 Step 17800 training loss = 8.781 validation loss = 8.683540 (minibatch size = 1000)\n",
      "Epoch 43 Step 17900 training loss = 8.839 validation loss = 8.703846 (minibatch size = 1000)\n",
      "Epoch 44 Step 18000 training loss = 8.785 validation loss = 8.682847 (minibatch size = 1000)\n",
      "Epoch 44 Step 18100 training loss = 8.803 validation loss = 8.692348 (minibatch size = 1000)\n",
      "Epoch 44 Step 18200 training loss = 8.762 validation loss = 8.689796 (minibatch size = 1000)\n",
      "Epoch 44 Step 18300 training loss = 8.826 validation loss = 8.709944 (minibatch size = 1000)\n",
      "Epoch 45 Step 18400 training loss = 8.822 validation loss = 8.686807 (minibatch size = 1000)\n",
      "Epoch 45 Step 18500 training loss = 8.787 validation loss = 8.682833 (minibatch size = 1000)\n",
      "Epoch 45 Step 18600 training loss = 8.758 validation loss = 8.697834 (minibatch size = 1000)\n",
      "Epoch 45 Step 18700 training loss = 8.824 validation loss = 8.681807 (minibatch size = 1000)\n",
      "Epoch 45 Step 18800 training loss = 8.767 validation loss = 8.683343 (minibatch size = 1000)\n",
      "Epoch 46 Step 18900 training loss = 8.827 validation loss = 8.677896 (minibatch size = 1000)\n",
      "Epoch 46 Step 19000 training loss = 8.757 validation loss = 8.681692 (minibatch size = 1000)\n",
      "Epoch 46 Step 19100 training loss = 8.822 validation loss = 8.697218 (minibatch size = 1000)\n",
      "Epoch 46 Step 19200 training loss = 8.789 validation loss = 8.699259 (minibatch size = 1000)\n",
      "Epoch 47 Step 19300 training loss = 8.763 validation loss = 8.690650 (minibatch size = 1000)\n",
      "Epoch 47 Step 19400 training loss = 8.787 validation loss = 8.677037 (minibatch size = 1000)\n",
      "Epoch 47 Step 19500 training loss = 8.822 validation loss = 8.703273 (minibatch size = 1000)\n",
      "Epoch 47 Step 19600 training loss = 8.832 validation loss = 8.682327 (minibatch size = 1000)\n",
      "Epoch 48 Step 19700 training loss = 8.783 validation loss = 8.678751 (minibatch size = 1000)\n",
      "Epoch 48 Step 19800 training loss = 8.788 validation loss = 8.701214 (minibatch size = 1000)\n",
      "Epoch 48 Step 19900 training loss = 8.764 validation loss = 8.674085 (minibatch size = 1000)\n",
      "Epoch 48 Step 20000 training loss = 8.823 validation loss = 8.668808 (minibatch size = 1000)\n",
      "Epoch 49 Step 20100 training loss = 8.781 validation loss = 8.677950 (minibatch size = 1000)\n",
      "Epoch 49 Step 20200 training loss = 8.791 validation loss = 8.682852 (minibatch size = 1000)\n",
      "Epoch 49 Step 20300 training loss = 8.754 validation loss = 8.683941 (minibatch size = 1000)\n",
      "Epoch 49 Step 20400 training loss = 8.838 validation loss = 8.689619 (minibatch size = 1000)\n",
      "Epoch 50 Step 20500 training loss = 8.793 validation loss = 8.695575 (minibatch size = 1000)\n",
      "Epoch 50 Step 20600 training loss = 8.792 validation loss = 8.680441 (minibatch size = 1000)\n",
      "Epoch 50 Step 20700 training loss = 8.751 validation loss = 8.675758 (minibatch size = 1000)\n",
      "Epoch 50 Step 20800 training loss = 8.818 validation loss = 8.689490 (minibatch size = 1000)\n",
      "Epoch 50 Step 20900 training loss = 8.806 validation loss = 8.683807 (minibatch size = 344)\n",
      "Epoch 51 Step 21000 training loss = 8.796 validation loss = 8.682023 (minibatch size = 1000)\n",
      "Epoch 51 Step 21100 training loss = 8.731 validation loss = 8.679688 (minibatch size = 1000)\n",
      "Epoch 51 Step 21200 training loss = 8.840 validation loss = 8.682241 (minibatch size = 1000)\n",
      "Epoch 51 Step 21300 training loss = 8.775 validation loss = 8.683652 (minibatch size = 1000)\n",
      "Epoch 52 Step 21400 training loss = 8.832 validation loss = 8.680582 (minibatch size = 1000)\n",
      "Epoch 52 Step 21500 training loss = 8.771 validation loss = 8.694316 (minibatch size = 1000)\n",
      "Epoch 52 Step 21600 training loss = 8.803 validation loss = 8.685205 (minibatch size = 1000)\n",
      "Epoch 52 Step 21700 training loss = 8.805 validation loss = 8.668003 (minibatch size = 1000)\n",
      "Epoch 53 Step 21800 training loss = 8.757 validation loss = 8.674330 (minibatch size = 1000)\n",
      "Epoch 53 Step 21900 training loss = 8.821 validation loss = 8.672147 (minibatch size = 1000)\n",
      "Epoch 53 Step 22000 training loss = 8.755 validation loss = 8.681843 (minibatch size = 1000)\n",
      "Epoch 53 Step 22100 training loss = 8.838 validation loss = 8.668468 (minibatch size = 1000)\n",
      "Epoch 54 Step 22200 training loss = 8.764 validation loss = 8.684975 (minibatch size = 1000)\n",
      "Epoch 54 Step 22300 training loss = 8.785 validation loss = 8.678114 (minibatch size = 1000)\n",
      "Epoch 54 Step 22400 training loss = 8.761 validation loss = 8.675261 (minibatch size = 1000)\n",
      "Epoch 54 Step 22500 training loss = 8.844 validation loss = 8.678144 (minibatch size = 1000)\n",
      "Epoch 55 Step 22600 training loss = 8.783 validation loss = 8.672030 (minibatch size = 1000)\n",
      "Epoch 55 Step 22700 training loss = 8.799 validation loss = 8.687870 (minibatch size = 1000)\n",
      "Epoch 55 Step 22800 training loss = 8.756 validation loss = 8.674041 (minibatch size = 1000)\n",
      "Epoch 55 Step 22900 training loss = 8.826 validation loss = 8.688570 (minibatch size = 1000)\n",
      "Epoch 56 Step 23000 training loss = 8.793 validation loss = 8.674148 (minibatch size = 1000)\n",
      "Epoch 56 Step 23100 training loss = 8.799 validation loss = 8.676239 (minibatch size = 1000)\n",
      "Epoch 56 Step 23200 training loss = 8.748 validation loss = 8.687869 (minibatch size = 1000)\n",
      "Epoch 56 Step 23300 training loss = 8.804 validation loss = 8.680774 (minibatch size = 1000)\n",
      "Epoch 56 Step 23400 training loss = 8.778 validation loss = 8.677605 (minibatch size = 1000)\n",
      "Epoch 57 Step 23500 training loss = 8.818 validation loss = 8.674241 (minibatch size = 1000)\n",
      "Epoch 57 Step 23600 training loss = 8.749 validation loss = 8.667389 (minibatch size = 1000)\n",
      "Epoch 57 Step 23700 training loss = 8.806 validation loss = 8.684346 (minibatch size = 1000)\n",
      "Epoch 57 Step 23800 training loss = 8.765 validation loss = 8.682438 (minibatch size = 1000)\n",
      "Epoch 58 Step 23900 training loss = 8.784 validation loss = 8.678182 (minibatch size = 1000)\n",
      "Epoch 58 Step 24000 training loss = 8.776 validation loss = 8.672504 (minibatch size = 1000)\n",
      "Epoch 58 Step 24100 training loss = 8.777 validation loss = 8.693008 (minibatch size = 1000)\n",
      "Epoch 58 Step 24200 training loss = 8.820 validation loss = 8.683304 (minibatch size = 1000)\n",
      "Epoch 59 Step 24300 training loss = 8.760 validation loss = 8.674746 (minibatch size = 1000)\n",
      "Epoch 59 Step 24400 training loss = 8.798 validation loss = 8.680660 (minibatch size = 1000)\n",
      "Epoch 59 Step 24500 training loss = 8.762 validation loss = 8.674454 (minibatch size = 1000)\n",
      "Epoch 59 Step 24600 training loss = 8.814 validation loss = 8.667579 (minibatch size = 1000)\n",
      "Epoch 60 Step 24700 training loss = 8.769 validation loss = 8.674167 (minibatch size = 1000)\n",
      "Epoch 60 Step 24800 training loss = 8.792 validation loss = 8.689276 (minibatch size = 1000)\n",
      "Epoch 60 Step 24900 training loss = 8.743 validation loss = 8.668075 (minibatch size = 1000)\n",
      "Epoch 60 Step 25000 training loss = 8.820 validation loss = 8.679132 (minibatch size = 1000)\n",
      "Epoch 61 Step 25100 training loss = 8.798 validation loss = 8.680976 (minibatch size = 1000)\n",
      "Epoch 61 Step 25200 training loss = 8.787 validation loss = 8.677421 (minibatch size = 1000)\n",
      "Epoch 61 Step 25300 training loss = 8.758 validation loss = 8.672633 (minibatch size = 1000)\n",
      "Epoch 61 Step 25400 training loss = 8.822 validation loss = 8.695374 (minibatch size = 1000)\n",
      "Epoch 62 Step 25500 training loss = 8.810 validation loss = 8.678391 (minibatch size = 1000)\n",
      "Epoch 62 Step 25600 training loss = 8.808 validation loss = 8.681791 (minibatch size = 1000)\n",
      "Epoch 62 Step 25700 training loss = 8.715 validation loss = 8.672687 (minibatch size = 1000)\n",
      "Epoch 62 Step 25800 training loss = 8.825 validation loss = 8.665503 (minibatch size = 1000)\n",
      "Epoch 62 Step 25900 training loss = 8.766 validation loss = 8.681592 (minibatch size = 1000)\n",
      "Epoch 63 Step 26000 training loss = 8.802 validation loss = 8.666848 (minibatch size = 1000)\n",
      "Epoch 63 Step 26100 training loss = 8.738 validation loss = 8.683922 (minibatch size = 1000)\n",
      "Epoch 63 Step 26200 training loss = 8.812 validation loss = 8.684325 (minibatch size = 1000)\n",
      "Epoch 63 Step 26300 training loss = 8.798 validation loss = 8.683180 (minibatch size = 1000)\n",
      "Epoch 64 Step 26400 training loss = 8.748 validation loss = 8.668119 (minibatch size = 1000)\n",
      "Epoch 64 Step 26500 training loss = 8.792 validation loss = 8.670300 (minibatch size = 1000)\n",
      "Epoch 64 Step 26600 training loss = 8.796 validation loss = 8.677138 (minibatch size = 1000)\n",
      "Epoch 64 Step 26700 training loss = 8.791 validation loss = 8.672463 (minibatch size = 1000)\n",
      "Epoch 65 Step 26800 training loss = 8.768 validation loss = 8.672024 (minibatch size = 1000)\n",
      "Epoch 65 Step 26900 training loss = 8.777 validation loss = 8.670613 (minibatch size = 1000)\n",
      "Epoch 65 Step 27000 training loss = 8.753 validation loss = 8.669205 (minibatch size = 1000)\n",
      "Epoch 65 Step 27100 training loss = 8.803 validation loss = 8.677224 (minibatch size = 1000)\n",
      "Epoch 66 Step 27200 training loss = 8.772 validation loss = 8.670826 (minibatch size = 1000)\n",
      "Epoch 66 Step 27300 training loss = 8.774 validation loss = 8.674868 (minibatch size = 1000)\n",
      "Epoch 66 Step 27400 training loss = 8.763 validation loss = 8.679625 (minibatch size = 1000)\n",
      "Epoch 66 Step 27500 training loss = 8.826 validation loss = 8.690114 (minibatch size = 1000)\n",
      "Epoch 67 Step 27600 training loss = 8.794 validation loss = 8.673227 (minibatch size = 1000)\n",
      "Epoch 67 Step 27700 training loss = 8.782 validation loss = 8.671515 (minibatch size = 1000)\n",
      "Epoch 67 Step 27800 training loss = 8.728 validation loss = 8.675471 (minibatch size = 1000)\n",
      "Epoch 67 Step 27900 training loss = 8.798 validation loss = 8.678920 (minibatch size = 1000)\n",
      "Epoch 67 Step 28000 training loss = 8.762 validation loss = 8.671028 (minibatch size = 1000)\n",
      "Epoch 68 Step 28100 training loss = 8.801 validation loss = 8.668584 (minibatch size = 1000)\n",
      "Epoch 68 Step 28200 training loss = 8.728 validation loss = 8.662755 (minibatch size = 1000)\n",
      "Epoch 68 Step 28300 training loss = 8.798 validation loss = 8.677798 (minibatch size = 1000)\n",
      "Epoch 68 Step 28400 training loss = 8.775 validation loss = 8.685028 (minibatch size = 1000)\n",
      "Epoch 69 Step 28500 training loss = 8.782 validation loss = 8.675965 (minibatch size = 1000)\n",
      "Epoch 69 Step 28600 training loss = 8.776 validation loss = 8.679986 (minibatch size = 1000)\n",
      "Epoch 69 Step 28700 training loss = 8.797 validation loss = 8.693994 (minibatch size = 1000)\n",
      "Epoch 69 Step 28800 training loss = 8.814 validation loss = 8.670495 (minibatch size = 1000)\n",
      "Epoch 70 Step 28900 training loss = 8.736 validation loss = 8.671573 (minibatch size = 1000)\n",
      "Epoch 70 Step 29000 training loss = 8.795 validation loss = 8.682824 (minibatch size = 1000)\n",
      "Epoch 70 Step 29100 training loss = 8.739 validation loss = 8.660134 (minibatch size = 1000)\n",
      "Epoch 70 Step 29200 training loss = 8.811 validation loss = 8.660524 (minibatch size = 1000)\n",
      "Epoch 71 Step 29300 training loss = 8.765 validation loss = 8.666824 (minibatch size = 1000)\n",
      "Epoch 71 Step 29400 training loss = 8.788 validation loss = 8.672772 (minibatch size = 1000)\n",
      "Epoch 71 Step 29500 training loss = 8.728 validation loss = 8.656743 (minibatch size = 1000)\n",
      "Epoch 71 Step 29600 training loss = 8.827 validation loss = 8.672593 (minibatch size = 1000)\n",
      "Epoch 72 Step 29700 training loss = 8.803 validation loss = 8.673473 (minibatch size = 1000)\n",
      "Epoch 72 Step 29800 training loss = 8.782 validation loss = 8.671174 (minibatch size = 1000)\n",
      "Epoch 72 Step 29900 training loss = 8.730 validation loss = 8.664508 (minibatch size = 1000)\n",
      "Epoch 72 Step 30000 training loss = 8.815 validation loss = 8.672754 (minibatch size = 1000)\n",
      "Epoch 73 Step 30100 training loss = 8.775 validation loss = 8.664057 (minibatch size = 1000)\n",
      "Epoch 73 Step 30200 training loss = 8.798 validation loss = 8.671845 (minibatch size = 1000)\n",
      "Epoch 73 Step 30300 training loss = 8.701 validation loss = 8.685702 (minibatch size = 1000)\n",
      "Epoch 73 Step 30400 training loss = 8.795 validation loss = 8.671100 (minibatch size = 1000)\n",
      "Epoch 73 Step 30500 training loss = 8.763 validation loss = 8.666134 (minibatch size = 1000)\n",
      "Epoch 74 Step 30600 training loss = 8.806 validation loss = 8.666680 (minibatch size = 1000)\n",
      "Epoch 74 Step 30700 training loss = 8.739 validation loss = 8.677816 (minibatch size = 1000)\n",
      "Epoch 74 Step 30800 training loss = 8.810 validation loss = 8.680722 (minibatch size = 1000)\n",
      "Epoch 74 Step 30900 training loss = 8.779 validation loss = 8.675425 (minibatch size = 1000)\n",
      "Epoch 75 Step 31000 training loss = 8.751 validation loss = 8.674226 (minibatch size = 1000)\n",
      "Epoch 75 Step 31100 training loss = 8.777 validation loss = 8.666812 (minibatch size = 1000)\n",
      "Epoch 75 Step 31200 training loss = 8.770 validation loss = 8.663049 (minibatch size = 1000)\n",
      "Epoch 75 Step 31300 training loss = 8.794 validation loss = 8.656585 (minibatch size = 1000)\n",
      "Epoch 76 Step 31400 training loss = 8.775 validation loss = 8.657908 (minibatch size = 1000)\n",
      "Epoch 76 Step 31500 training loss = 8.779 validation loss = 8.670243 (minibatch size = 1000)\n",
      "Epoch 76 Step 31600 training loss = 8.738 validation loss = 8.665893 (minibatch size = 1000)\n",
      "Epoch 76 Step 31700 training loss = 8.825 validation loss = 8.662601 (minibatch size = 1000)\n",
      "Epoch 77 Step 31800 training loss = 8.746 validation loss = 8.651268 (minibatch size = 1000)\n",
      "Epoch 77 Step 31900 training loss = 8.770 validation loss = 8.668190 (minibatch size = 1000)\n",
      "Epoch 77 Step 32000 training loss = 8.731 validation loss = 8.660468 (minibatch size = 1000)\n",
      "Epoch 77 Step 32100 training loss = 8.810 validation loss = 8.673718 (minibatch size = 1000)\n",
      "Epoch 78 Step 32200 training loss = 8.775 validation loss = 8.662507 (minibatch size = 1000)\n",
      "Epoch 78 Step 32300 training loss = 8.766 validation loss = 8.665353 (minibatch size = 1000)\n",
      "Epoch 78 Step 32400 training loss = 8.733 validation loss = 8.664915 (minibatch size = 1000)\n",
      "Epoch 78 Step 32500 training loss = 8.789 validation loss = 8.669228 (minibatch size = 1000)\n",
      "Epoch 78 Step 32600 training loss = 8.771 validation loss = 8.677163 (minibatch size = 1000)\n",
      "Epoch 79 Step 32700 training loss = 8.786 validation loss = 8.672274 (minibatch size = 1000)\n",
      "Epoch 79 Step 32800 training loss = 8.726 validation loss = 8.668571 (minibatch size = 1000)\n",
      "Epoch 79 Step 32900 training loss = 8.822 validation loss = 8.673576 (minibatch size = 1000)\n",
      "Epoch 79 Step 33000 training loss = 8.750 validation loss = 8.671965 (minibatch size = 1000)\n",
      "Epoch 80 Step 33100 training loss = 8.763 validation loss = 8.664688 (minibatch size = 1000)\n",
      "Epoch 80 Step 33200 training loss = 8.734 validation loss = 8.672043 (minibatch size = 1000)\n",
      "Epoch 80 Step 33300 training loss = 8.788 validation loss = 8.667161 (minibatch size = 1000)\n",
      "Epoch 80 Step 33400 training loss = 8.803 validation loss = 8.659981 (minibatch size = 1000)\n",
      "Epoch 81 Step 33500 training loss = 8.736 validation loss = 8.663347 (minibatch size = 1000)\n",
      "Epoch 81 Step 33600 training loss = 8.799 validation loss = 8.659307 (minibatch size = 1000)\n",
      "Epoch 81 Step 33700 training loss = 8.719 validation loss = 8.653318 (minibatch size = 1000)\n",
      "Epoch 81 Step 33800 training loss = 8.819 validation loss = 8.655141 (minibatch size = 1000)\n",
      "Epoch 82 Step 33900 training loss = 8.752 validation loss = 8.650182 (minibatch size = 1000)\n",
      "Epoch 82 Step 34000 training loss = 8.767 validation loss = 8.657946 (minibatch size = 1000)\n",
      "Epoch 82 Step 34100 training loss = 8.753 validation loss = 8.651727 (minibatch size = 1000)\n",
      "Epoch 82 Step 34200 training loss = 8.789 validation loss = 8.667496 (minibatch size = 1000)\n",
      "Epoch 83 Step 34300 training loss = 8.773 validation loss = 8.679569 (minibatch size = 1000)\n",
      "Epoch 83 Step 34400 training loss = 8.772 validation loss = 8.664289 (minibatch size = 1000)\n",
      "Epoch 83 Step 34500 training loss = 8.725 validation loss = 8.663198 (minibatch size = 1000)\n",
      "Epoch 83 Step 34600 training loss = 8.795 validation loss = 8.675371 (minibatch size = 1000)\n",
      "Epoch 84 Step 34700 training loss = 8.792 validation loss = 8.660714 (minibatch size = 1000)\n",
      "Epoch 84 Step 34800 training loss = 8.774 validation loss = 8.668628 (minibatch size = 1000)\n",
      "Epoch 84 Step 34900 training loss = 8.736 validation loss = 8.677316 (minibatch size = 1000)\n",
      "Epoch 84 Step 35000 training loss = 8.784 validation loss = 8.652327 (minibatch size = 1000)\n",
      "Epoch 84 Step 35100 training loss = 8.767 validation loss = 8.659273 (minibatch size = 1000)\n",
      "Epoch 85 Step 35200 training loss = 8.799 validation loss = 8.651403 (minibatch size = 1000)\n",
      "Epoch 85 Step 35300 training loss = 8.727 validation loss = 8.664459 (minibatch size = 1000)\n",
      "Epoch 85 Step 35400 training loss = 8.782 validation loss = 8.668139 (minibatch size = 1000)\n",
      "Epoch 85 Step 35500 training loss = 8.764 validation loss = 8.657018 (minibatch size = 1000)\n",
      "Epoch 86 Step 35600 training loss = 8.741 validation loss = 8.654073 (minibatch size = 1000)\n",
      "Epoch 86 Step 35700 training loss = 8.756 validation loss = 8.659399 (minibatch size = 1000)\n",
      "Epoch 86 Step 35800 training loss = 8.755 validation loss = 8.667171 (minibatch size = 1000)\n",
      "Epoch 86 Step 35900 training loss = 8.795 validation loss = 8.662040 (minibatch size = 1000)\n",
      "Epoch 87 Step 36000 training loss = 8.755 validation loss = 8.654404 (minibatch size = 1000)\n",
      "Epoch 87 Step 36100 training loss = 8.769 validation loss = 8.673306 (minibatch size = 1000)\n",
      "Epoch 87 Step 36200 training loss = 8.723 validation loss = 8.649984 (minibatch size = 1000)\n",
      "Epoch 87 Step 36300 training loss = 8.814 validation loss = 8.664308 (minibatch size = 1000)\n",
      "Epoch 88 Step 36400 training loss = 8.758 validation loss = 8.657088 (minibatch size = 1000)\n",
      "Epoch 88 Step 36500 training loss = 8.768 validation loss = 8.650356 (minibatch size = 1000)\n",
      "Epoch 88 Step 36600 training loss = 8.725 validation loss = 8.659485 (minibatch size = 1000)\n",
      "Epoch 88 Step 36700 training loss = 8.800 validation loss = 8.662741 (minibatch size = 1000)\n",
      "Epoch 89 Step 36800 training loss = 8.789 validation loss = 8.663940 (minibatch size = 1000)\n",
      "Epoch 89 Step 36900 training loss = 8.769 validation loss = 8.655486 (minibatch size = 1000)\n",
      "Epoch 89 Step 37000 training loss = 8.722 validation loss = 8.652651 (minibatch size = 1000)\n",
      "Epoch 89 Step 37100 training loss = 8.786 validation loss = 8.666241 (minibatch size = 1000)\n",
      "Epoch 89 Step 37200 training loss = 8.743 validation loss = 8.663291 (minibatch size = 1000)\n",
      "Epoch 90 Step 37300 training loss = 8.791 validation loss = 8.660082 (minibatch size = 1000)\n",
      "Epoch 90 Step 37400 training loss = 8.714 validation loss = 8.662417 (minibatch size = 1000)\n",
      "Epoch 90 Step 37500 training loss = 8.811 validation loss = 8.660290 (minibatch size = 1000)\n",
      "Epoch 90 Step 37600 training loss = 8.738 validation loss = 8.664638 (minibatch size = 1000)\n",
      "Epoch 91 Step 37700 training loss = 8.772 validation loss = 8.667826 (minibatch size = 1000)\n",
      "Epoch 91 Step 37800 training loss = 8.728 validation loss = 8.676497 (minibatch size = 1000)\n",
      "Epoch 91 Step 37900 training loss = 8.770 validation loss = 8.677241 (minibatch size = 1000)\n",
      "Epoch 91 Step 38000 training loss = 8.779 validation loss = 8.661846 (minibatch size = 1000)\n",
      "Epoch 92 Step 38100 training loss = 8.733 validation loss = 8.655872 (minibatch size = 1000)\n",
      "Epoch 92 Step 38200 training loss = 8.775 validation loss = 8.662569 (minibatch size = 1000)\n",
      "Epoch 92 Step 38300 training loss = 8.739 validation loss = 8.661963 (minibatch size = 1000)\n",
      "Epoch 92 Step 38400 training loss = 8.806 validation loss = 8.656785 (minibatch size = 1000)\n",
      "Epoch 93 Step 38500 training loss = 8.750 validation loss = 8.664120 (minibatch size = 1000)\n",
      "Epoch 93 Step 38600 training loss = 8.765 validation loss = 8.664803 (minibatch size = 1000)\n",
      "Epoch 93 Step 38700 training loss = 8.730 validation loss = 8.656966 (minibatch size = 1000)\n",
      "Epoch 93 Step 38800 training loss = 8.798 validation loss = 8.671535 (minibatch size = 1000)\n",
      "Epoch 94 Step 38900 training loss = 8.757 validation loss = 8.662656 (minibatch size = 1000)\n",
      "Epoch 94 Step 39000 training loss = 8.771 validation loss = 8.658633 (minibatch size = 1000)\n",
      "Epoch 94 Step 39100 training loss = 8.731 validation loss = 8.661864 (minibatch size = 1000)\n",
      "Epoch 94 Step 39200 training loss = 8.801 validation loss = 8.686630 (minibatch size = 1000)\n",
      "Epoch 95 Step 39300 training loss = 8.788 validation loss = 8.667802 (minibatch size = 1000)\n",
      "Epoch 95 Step 39400 training loss = 8.754 validation loss = 8.669047 (minibatch size = 1000)\n",
      "Epoch 95 Step 39500 training loss = 8.712 validation loss = 8.678522 (minibatch size = 1000)\n",
      "Epoch 95 Step 39600 training loss = 8.758 validation loss = 8.666263 (minibatch size = 1000)\n",
      "Epoch 95 Step 39700 training loss = 8.763 validation loss = 8.662910 (minibatch size = 1000)\n",
      "Epoch 96 Step 39800 training loss = 8.793 validation loss = 8.648094 (minibatch size = 1000)\n",
      "Epoch 96 Step 39900 training loss = 8.725 validation loss = 8.662912 (minibatch size = 1000)\n",
      "Epoch 96 Step 40000 training loss = 8.788 validation loss = 8.673732 (minibatch size = 1000)\n",
      "Epoch 96 Step 40100 training loss = 8.749 validation loss = 8.672469 (minibatch size = 1000)\n",
      "Epoch 97 Step 40200 training loss = 8.746 validation loss = 8.656377 (minibatch size = 1000)\n",
      "Epoch 97 Step 40300 training loss = 8.744 validation loss = 8.657540 (minibatch size = 1000)\n",
      "Epoch 97 Step 40400 training loss = 8.747 validation loss = 8.682746 (minibatch size = 1000)\n",
      "Epoch 97 Step 40500 training loss = 8.792 validation loss = 8.655167 (minibatch size = 1000)\n",
      "Epoch 98 Step 40600 training loss = 8.746 validation loss = 8.661162 (minibatch size = 1000)\n",
      "Epoch 98 Step 40700 training loss = 8.751 validation loss = 8.677387 (minibatch size = 1000)\n",
      "Epoch 98 Step 40800 training loss = 8.730 validation loss = 8.663157 (minibatch size = 1000)\n",
      "Epoch 98 Step 40900 training loss = 8.775 validation loss = 8.660186 (minibatch size = 1000)\n",
      "Epoch 99 Step 41000 training loss = 8.760 validation loss = 8.655499 (minibatch size = 1000)\n",
      "Epoch 99 Step 41100 training loss = 8.752 validation loss = 8.661518 (minibatch size = 1000)\n",
      "Epoch 99 Step 41200 training loss = 8.723 validation loss = 8.655515 (minibatch size = 1000)\n",
      "Epoch 99 Step 41300 training loss = 8.806 validation loss = 8.668433 (minibatch size = 1000)\n",
      "Epoch 100 Step 41400 training loss = 8.755 validation loss = 8.651880 (minibatch size = 1000)\n",
      "Epoch 100 Step 41500 training loss = 8.764 validation loss = 8.657853 (minibatch size = 1000)\n",
      "Epoch 100 Step 41600 training loss = 8.717 validation loss = 8.648267 (minibatch size = 1000)\n",
      "Epoch 100 Step 41700 training loss = 8.776 validation loss = 8.666250 (minibatch size = 1000)\n",
      "Epoch 100 Step 41800 training loss = 8.756 validation loss = 8.657990 (minibatch size = 344)\n",
      "==================================================result==================================================\n",
      "the best step is 39800 with minimum validation error = 8.648094177246094\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H = 90\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "Q5_model = myMLP()\n",
    "net = Q5_model.Net(input_shape, device, H, dropout = True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "model = Q5_model.train(device, net, nepoch, log_interval, optimizer, subtrainloader, validloader, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1608734538630,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "0hOdhG4fprk_",
    "outputId": "4c35dbc7-817c-43a1-adb1-06084f51cc8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 90, test_RMSE = 8.87172010696875, with droupout and optimizer = Adam\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_RMSE = Q5_model.test(device, H, model, testloader)\n",
    "print('when H = {}, test_RMSE = {}, with droupout and optimizer = Adam'.format(H, test_RMSE))\n",
    "Q5_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從圖形中可以看書training error和validation error都有降下，但training error比validation高。顯示出模型受到drop out的影響，因為在訓練的過程內設計drop out，只留下一部分的feature來產生output，會導致留下來的feature對output影響結果變強。等到進入validation階段的時候，全部的neuron一起運作就會讓output的結果更好(error更小)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXRXjXfuD1Ky"
   },
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1206168,
     "status": "ok",
     "timestamp": 1608725652171,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "0m0WvZRLDpFP",
    "outputId": "43500313-8e68-427a-b4ef-deafafaf5e69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 20, test_RMSE = 9.111369019732795\n",
      "when H = 45, test_RMSE = 8.942904964925495\n",
      "when H = 180, test_RMSE = 8.845475334009166\n",
      "when H = 360, test_RMSE = 8.832860358895017\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H_list = [20, 45, 180, 360]\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "\n",
    "for H in H_list:\n",
    "    Q6_model = myMLP()\n",
    "    net = Q6_model.Net(input_shape, device, H, dropout = True)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    model = Q6_model.train(device, net, nepoch, log_interval, optimizer, subtrainloader, validloader, verbose = False)\n",
    "    test_RMSE = Q6_model.test(device, H, model, testloader)\n",
    "    print('when H = {}, test_RMSE = {}'.format(H, test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "與Q2-Q5的結果不同，在有drop out及使用Adam演算法的結果之下，H越大反而越不會overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04JQtk7HI7gQ"
   },
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1608735809977,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "P07yJip9qnr3"
   },
   "outputs": [],
   "source": [
    "class myMLP_2():\n",
    "    def __init__(self):\n",
    "        self.batch_num = None\n",
    "        self.subtrainRMSE = list()\n",
    "        self.validRMSE = list()\n",
    "        self.mse = torch.nn.MSELoss(reduction = 'mean')\n",
    "        self.sse = torch.nn.MSELoss(reduction = 'sum')\n",
    "        self.L1 = torch.nn.L1Loss(reduction = 'sum')\n",
    "        self.net = None\n",
    "\n",
    "    def Net(self, input_shape, device, H = 45, dropout = False):\n",
    "        D_in = input_shape\n",
    "        H = H\n",
    "        D_out = 1\n",
    "        if dropout == False:\n",
    "              net = torch.nn.Sequential(\n",
    "                  torch.nn.Linear(D_in, H),  \n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Linear(H, H),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Linear(H, H),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Linear(H, H),\n",
    "                  torch.nn.ReLU(),\n",
    "                  torch.nn.Linear(H, D_out)\n",
    "              )\n",
    "\n",
    "        if dropout == True:\n",
    "            net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(D_in, H),  \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p = 0.5),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p = 0.5),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p = 0.5),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p = 0.5),\n",
    "            torch.nn.Linear(H, D_out)\n",
    "            )\n",
    "\n",
    "        # convert everything to float precision. \n",
    "        net = net.float()\n",
    "        # move the model to device (i.e., cpu or gpu)\n",
    "        net = net.to(device)\n",
    "        return net\n",
    "\n",
    "  # Train\n",
    "    def train(self, device, model, nepoch, log_interval, optimizer, z, train_loader, valid_loader, verbose = True, customized_loss = False):\n",
    "\n",
    "        # Early stopping\n",
    "        min_valid_loss = 10000000\n",
    "        best_step_count = 0\n",
    "        patience = 5000\n",
    "    \n",
    "        sum_loss_MSE = 0\n",
    "        step_count = 0\n",
    "        for epoch_idx in range(1, nepoch+1):\n",
    "          #1 epoch = 400 batch (1 batch = 1000 data points)    \n",
    "          for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            #reshape target to two-dimensional array\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            step_count += 1        \n",
    "            model.train()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #loss for update        \n",
    "            loss_SSE = self.sse(outputs, targets)\n",
    "            loss_L1 = self.L1(outputs, targets)\n",
    "            if customized_loss == True:\n",
    "                total_loss = z * loss_SSE + (1-z) * 0.5 * loss_L1\n",
    "            else:\n",
    "                total_loss = z * loss_SSE + (1-z) * loss_L1\n",
    "            #loss for show\n",
    "            loss_MSE = self.mse(outputs, targets)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss_MSE += loss_MSE\n",
    "            if step_count % log_interval == 0:\n",
    "            #1000 = batch_size, 100 = number of batch         \n",
    "                RMSE_train = (sum_loss_MSE / log_interval) ** (1/2)\n",
    "                RMSE_valid = self.validation(model, device, log_interval, valid_loader)\n",
    "                self.subtrainRMSE.append(RMSE_train)\n",
    "                self.validRMSE.append(RMSE_valid)\n",
    "                if verbose == True:\n",
    "                    print(\"Epoch %d Step %d training loss = %.3f validation loss = %3f (minibatch size = %d)\" % (epoch_idx, step_count, RMSE_train, RMSE_valid, len(targets))) \n",
    "\n",
    "                if RMSE_valid < min_valid_loss:\n",
    "                    best_step_count = step_count\n",
    "                    min_valid_loss = RMSE_valid\n",
    "                    best_model = model\n",
    "                sum_loss_MSE = 0 \n",
    "\n",
    "            if step_count >= best_step_count + patience and RMSE_valid > min_valid_loss:\n",
    "                if verbose == True:\n",
    "                    print('Early stopping!')\n",
    "                    print('='*50 + 'validation result' + '='*50)\n",
    "                    print('the best step is {} with minimum validation error = {}'.format(best_step_count, min_valid_loss))\n",
    "                    self.batch_num = step_count\n",
    "                return best_model\n",
    "        if verbose == True:\n",
    "            print('='*50 + 'result' + '='*50)\n",
    "            print('the best step is {} with minimum validation error = {}'.format(best_step_count, min_valid_loss))\n",
    "        self.batch_num = step_count\n",
    "        return best_model\n",
    "\n",
    "\n",
    "    def validation(self, model, device, log_interval, valid_loader):\n",
    "        #切到evaluate的模式，不會進行gradient descent\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            #1 epoch = 40 batch\n",
    "            for batch_idx, (inputs, targets) in enumerate(valid_loader):            \n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.reshape((-1, 1))\n",
    "                outputs = model(inputs)        \n",
    "                valid_loss = self.mse(outputs, targets)\n",
    "                #print(batch_idx, valid_loss)\n",
    "                total_loss += valid_loss\n",
    "        RMSE = (total_loss / batch_idx) ** (1/2)\n",
    "        #print(batch_idx)\n",
    "        return RMSE\n",
    "\n",
    "    def test(self, device, H, net, test_loader):\n",
    "        net.eval()\n",
    "        sum_test_MSE = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(test_loader):            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            outputs = net(inputs)        \n",
    "            test_loss = self.mse(outputs, targets).item()\n",
    "            sum_test_MSE += test_loss \n",
    "        test_RMSE = (sum_test_MSE / batch_idx) ** (1/2)\n",
    "        return test_RMSE\n",
    "  \n",
    "    def plot(self):\n",
    "        #print subtraining result\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        x = np.linspace(100, self.batch_num, self.batch_num // 100)\n",
    "        ax.plot(x, self.subtrainRMSE, label = 'subtrain RMSE')\n",
    "        ax.plot(x, self.validRMSE, label = 'validation RMSE')\n",
    "        ax.legend()\n",
    "        #plt.title('H = {}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 289494,
     "status": "ok",
     "timestamp": 1608736103748,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "pT1ELpu_tEQw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ba23933d-64fe-4863-f5cc-9588ba543479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 100 training loss = 10.541 validation loss = 9.562422 (minibatch size = 1000)\n",
      "Epoch 1 Step 200 training loss = 9.553 validation loss = 9.196228 (minibatch size = 1000)\n",
      "Epoch 1 Step 300 training loss = 9.429 validation loss = 9.061416 (minibatch size = 1000)\n",
      "Epoch 1 Step 400 training loss = 9.254 validation loss = 9.011939 (minibatch size = 1000)\n",
      "Epoch 2 Step 500 training loss = 9.250 validation loss = 8.945481 (minibatch size = 1000)\n",
      "Epoch 2 Step 600 training loss = 9.141 validation loss = 8.948561 (minibatch size = 1000)\n",
      "Epoch 2 Step 700 training loss = 9.162 validation loss = 8.919297 (minibatch size = 1000)\n",
      "Epoch 2 Step 800 training loss = 9.118 validation loss = 8.906487 (minibatch size = 1000)\n",
      "Epoch 3 Step 900 training loss = 9.069 validation loss = 8.882791 (minibatch size = 1000)\n",
      "Epoch 3 Step 1000 training loss = 9.100 validation loss = 8.854530 (minibatch size = 1000)\n",
      "Epoch 3 Step 1100 training loss = 9.036 validation loss = 8.850271 (minibatch size = 1000)\n",
      "Epoch 3 Step 1200 training loss = 9.093 validation loss = 8.844015 (minibatch size = 1000)\n",
      "Epoch 4 Step 1300 training loss = 9.021 validation loss = 8.835958 (minibatch size = 1000)\n",
      "Epoch 4 Step 1400 training loss = 9.023 validation loss = 8.842009 (minibatch size = 1000)\n",
      "Epoch 4 Step 1500 training loss = 9.005 validation loss = 8.806104 (minibatch size = 1000)\n",
      "Epoch 4 Step 1600 training loss = 9.057 validation loss = 8.813931 (minibatch size = 1000)\n",
      "Epoch 5 Step 1700 training loss = 8.985 validation loss = 8.800014 (minibatch size = 1000)\n",
      "Epoch 5 Step 1800 training loss = 8.995 validation loss = 8.807361 (minibatch size = 1000)\n",
      "Epoch 5 Step 1900 training loss = 8.969 validation loss = 8.787519 (minibatch size = 1000)\n",
      "Epoch 5 Step 2000 training loss = 9.028 validation loss = 8.809778 (minibatch size = 1000)\n",
      "Epoch 6 Step 2100 training loss = 8.986 validation loss = 8.789176 (minibatch size = 1000)\n",
      "Epoch 6 Step 2200 training loss = 8.975 validation loss = 8.777168 (minibatch size = 1000)\n",
      "Epoch 6 Step 2300 training loss = 8.935 validation loss = 8.798371 (minibatch size = 1000)\n",
      "Epoch 6 Step 2400 training loss = 8.961 validation loss = 8.769205 (minibatch size = 1000)\n",
      "Epoch 6 Step 2500 training loss = 8.950 validation loss = 8.787417 (minibatch size = 1000)\n",
      "Epoch 7 Step 2600 training loss = 8.976 validation loss = 8.773390 (minibatch size = 1000)\n",
      "Epoch 7 Step 2700 training loss = 8.917 validation loss = 8.766243 (minibatch size = 1000)\n",
      "Epoch 7 Step 2800 training loss = 8.980 validation loss = 8.758834 (minibatch size = 1000)\n",
      "Epoch 7 Step 2900 training loss = 8.939 validation loss = 8.770922 (minibatch size = 1000)\n",
      "Epoch 8 Step 3000 training loss = 8.928 validation loss = 8.760376 (minibatch size = 1000)\n",
      "Epoch 8 Step 3100 training loss = 8.901 validation loss = 8.752951 (minibatch size = 1000)\n",
      "Epoch 8 Step 3200 training loss = 8.942 validation loss = 8.786547 (minibatch size = 1000)\n",
      "Epoch 8 Step 3300 training loss = 8.954 validation loss = 8.757282 (minibatch size = 1000)\n",
      "Epoch 9 Step 3400 training loss = 8.906 validation loss = 8.750111 (minibatch size = 1000)\n",
      "Epoch 9 Step 3500 training loss = 8.945 validation loss = 8.762652 (minibatch size = 1000)\n",
      "Epoch 9 Step 3600 training loss = 8.896 validation loss = 8.746380 (minibatch size = 1000)\n",
      "Epoch 9 Step 3700 training loss = 8.969 validation loss = 8.728003 (minibatch size = 1000)\n",
      "Epoch 10 Step 3800 training loss = 8.892 validation loss = 8.734867 (minibatch size = 1000)\n",
      "Epoch 10 Step 3900 training loss = 8.909 validation loss = 8.742549 (minibatch size = 1000)\n",
      "Epoch 10 Step 4000 training loss = 8.875 validation loss = 8.732741 (minibatch size = 1000)\n",
      "Epoch 10 Step 4100 training loss = 8.952 validation loss = 8.743793 (minibatch size = 1000)\n",
      "Epoch 11 Step 4200 training loss = 8.899 validation loss = 8.743917 (minibatch size = 1000)\n",
      "Epoch 11 Step 4300 training loss = 8.910 validation loss = 8.732333 (minibatch size = 1000)\n",
      "Epoch 11 Step 4400 training loss = 8.860 validation loss = 8.725525 (minibatch size = 1000)\n",
      "Epoch 11 Step 4500 training loss = 8.943 validation loss = 8.752194 (minibatch size = 1000)\n",
      "Epoch 12 Step 4600 training loss = 8.884 validation loss = 8.737932 (minibatch size = 1000)\n",
      "Epoch 12 Step 4700 training loss = 8.915 validation loss = 8.748279 (minibatch size = 1000)\n",
      "Epoch 12 Step 4800 training loss = 8.830 validation loss = 8.731995 (minibatch size = 1000)\n",
      "Epoch 12 Step 4900 training loss = 8.952 validation loss = 8.712296 (minibatch size = 1000)\n",
      "Epoch 12 Step 5000 training loss = 8.865 validation loss = 8.729486 (minibatch size = 1000)\n",
      "Epoch 13 Step 5100 training loss = 8.920 validation loss = 8.710917 (minibatch size = 1000)\n",
      "Epoch 13 Step 5200 training loss = 8.842 validation loss = 8.736343 (minibatch size = 1000)\n",
      "Epoch 13 Step 5300 training loss = 8.921 validation loss = 8.732331 (minibatch size = 1000)\n",
      "Epoch 13 Step 5400 training loss = 8.907 validation loss = 8.737368 (minibatch size = 1000)\n",
      "Epoch 14 Step 5500 training loss = 8.855 validation loss = 8.723547 (minibatch size = 1000)\n",
      "Epoch 14 Step 5600 training loss = 8.895 validation loss = 8.716644 (minibatch size = 1000)\n",
      "Epoch 14 Step 5700 training loss = 8.868 validation loss = 8.723398 (minibatch size = 1000)\n",
      "Epoch 14 Step 5800 training loss = 8.898 validation loss = 8.708011 (minibatch size = 1000)\n",
      "Epoch 15 Step 5900 training loss = 8.875 validation loss = 8.715734 (minibatch size = 1000)\n",
      "Epoch 15 Step 6000 training loss = 8.877 validation loss = 8.719505 (minibatch size = 1000)\n",
      "Epoch 15 Step 6100 training loss = 8.856 validation loss = 8.715404 (minibatch size = 1000)\n",
      "Epoch 15 Step 6200 training loss = 8.908 validation loss = 8.714916 (minibatch size = 1000)\n",
      "Epoch 16 Step 6300 training loss = 8.868 validation loss = 8.710133 (minibatch size = 1000)\n",
      "Epoch 16 Step 6400 training loss = 8.866 validation loss = 8.720801 (minibatch size = 1000)\n",
      "Epoch 16 Step 6500 training loss = 8.873 validation loss = 8.710011 (minibatch size = 1000)\n",
      "Epoch 16 Step 6600 training loss = 8.908 validation loss = 8.737753 (minibatch size = 1000)\n",
      "Epoch 17 Step 6700 training loss = 8.892 validation loss = 8.732501 (minibatch size = 1000)\n",
      "Epoch 17 Step 6800 training loss = 8.862 validation loss = 8.719919 (minibatch size = 1000)\n",
      "Epoch 17 Step 6900 training loss = 8.819 validation loss = 8.723664 (minibatch size = 1000)\n",
      "Epoch 17 Step 7000 training loss = 8.874 validation loss = 8.720510 (minibatch size = 1000)\n",
      "Epoch 17 Step 7100 training loss = 8.857 validation loss = 8.718885 (minibatch size = 1000)\n",
      "Epoch 18 Step 7200 training loss = 8.877 validation loss = 8.713792 (minibatch size = 1000)\n",
      "Epoch 18 Step 7300 training loss = 8.791 validation loss = 8.711554 (minibatch size = 1000)\n",
      "Epoch 18 Step 7400 training loss = 8.903 validation loss = 8.710726 (minibatch size = 1000)\n",
      "Epoch 18 Step 7500 training loss = 8.846 validation loss = 8.731163 (minibatch size = 1000)\n",
      "Epoch 19 Step 7600 training loss = 8.863 validation loss = 8.722254 (minibatch size = 1000)\n",
      "Epoch 19 Step 7700 training loss = 8.828 validation loss = 8.725646 (minibatch size = 1000)\n",
      "Epoch 19 Step 7800 training loss = 8.891 validation loss = 8.737798 (minibatch size = 1000)\n",
      "Epoch 19 Step 7900 training loss = 8.883 validation loss = 8.720367 (minibatch size = 1000)\n",
      "Epoch 20 Step 8000 training loss = 8.814 validation loss = 8.715793 (minibatch size = 1000)\n",
      "Epoch 20 Step 8100 training loss = 8.857 validation loss = 8.713010 (minibatch size = 1000)\n",
      "Epoch 20 Step 8200 training loss = 8.811 validation loss = 8.709594 (minibatch size = 1000)\n",
      "Epoch 20 Step 8300 training loss = 8.899 validation loss = 8.700064 (minibatch size = 1000)\n",
      "Epoch 21 Step 8400 training loss = 8.852 validation loss = 8.705430 (minibatch size = 1000)\n",
      "Epoch 21 Step 8500 training loss = 8.849 validation loss = 8.708835 (minibatch size = 1000)\n",
      "Epoch 21 Step 8600 training loss = 8.831 validation loss = 8.692056 (minibatch size = 1000)\n",
      "Epoch 21 Step 8700 training loss = 8.899 validation loss = 8.711661 (minibatch size = 1000)\n",
      "Epoch 22 Step 8800 training loss = 8.827 validation loss = 8.707573 (minibatch size = 1000)\n",
      "Epoch 22 Step 8900 training loss = 8.855 validation loss = 8.706983 (minibatch size = 1000)\n",
      "Epoch 22 Step 9000 training loss = 8.808 validation loss = 8.714942 (minibatch size = 1000)\n",
      "Epoch 22 Step 9100 training loss = 8.902 validation loss = 8.736994 (minibatch size = 1000)\n",
      "Epoch 23 Step 9200 training loss = 8.853 validation loss = 8.709599 (minibatch size = 1000)\n",
      "Epoch 23 Step 9300 training loss = 8.854 validation loss = 8.720172 (minibatch size = 1000)\n",
      "Epoch 23 Step 9400 training loss = 8.800 validation loss = 8.725192 (minibatch size = 1000)\n",
      "Epoch 23 Step 9500 training loss = 8.884 validation loss = 8.704624 (minibatch size = 1000)\n",
      "Epoch 23 Step 9600 training loss = 8.825 validation loss = 8.702232 (minibatch size = 1000)\n",
      "Epoch 24 Step 9700 training loss = 8.858 validation loss = 8.689372 (minibatch size = 1000)\n",
      "Epoch 24 Step 9800 training loss = 8.801 validation loss = 8.717684 (minibatch size = 1000)\n",
      "Epoch 24 Step 9900 training loss = 8.882 validation loss = 8.714201 (minibatch size = 1000)\n",
      "Epoch 24 Step 10000 training loss = 8.839 validation loss = 8.706532 (minibatch size = 1000)\n",
      "Epoch 25 Step 10100 training loss = 8.797 validation loss = 8.707729 (minibatch size = 1000)\n",
      "Epoch 25 Step 10200 training loss = 8.843 validation loss = 8.683383 (minibatch size = 1000)\n",
      "Epoch 25 Step 10300 training loss = 8.845 validation loss = 8.707754 (minibatch size = 1000)\n",
      "Epoch 25 Step 10400 training loss = 8.863 validation loss = 8.698326 (minibatch size = 1000)\n",
      "Epoch 26 Step 10500 training loss = 8.831 validation loss = 8.695433 (minibatch size = 1000)\n",
      "Epoch 26 Step 10600 training loss = 8.817 validation loss = 8.713787 (minibatch size = 1000)\n",
      "Epoch 26 Step 10700 training loss = 8.806 validation loss = 8.693913 (minibatch size = 1000)\n",
      "Epoch 26 Step 10800 training loss = 8.871 validation loss = 8.696295 (minibatch size = 1000)\n",
      "Epoch 27 Step 10900 training loss = 8.816 validation loss = 8.696811 (minibatch size = 1000)\n",
      "Epoch 27 Step 11000 training loss = 8.841 validation loss = 8.708391 (minibatch size = 1000)\n",
      "Epoch 27 Step 11100 training loss = 8.782 validation loss = 8.693841 (minibatch size = 1000)\n",
      "Epoch 27 Step 11200 training loss = 8.882 validation loss = 8.707130 (minibatch size = 1000)\n",
      "Epoch 28 Step 11300 training loss = 8.816 validation loss = 8.696636 (minibatch size = 1000)\n",
      "Epoch 28 Step 11400 training loss = 8.846 validation loss = 8.703580 (minibatch size = 1000)\n",
      "Epoch 28 Step 11500 training loss = 8.772 validation loss = 8.703204 (minibatch size = 1000)\n",
      "Epoch 28 Step 11600 training loss = 8.856 validation loss = 8.699355 (minibatch size = 1000)\n",
      "Epoch 28 Step 11700 training loss = 8.820 validation loss = 8.694906 (minibatch size = 1000)\n",
      "Epoch 29 Step 11800 training loss = 8.849 validation loss = 8.703585 (minibatch size = 1000)\n",
      "Epoch 29 Step 11900 training loss = 8.754 validation loss = 8.690153 (minibatch size = 1000)\n",
      "Epoch 29 Step 12000 training loss = 8.877 validation loss = 8.688239 (minibatch size = 1000)\n",
      "Epoch 29 Step 12100 training loss = 8.810 validation loss = 8.696609 (minibatch size = 1000)\n",
      "Epoch 30 Step 12200 training loss = 8.837 validation loss = 8.696840 (minibatch size = 1000)\n",
      "Epoch 30 Step 12300 training loss = 8.791 validation loss = 8.710289 (minibatch size = 1000)\n",
      "Epoch 30 Step 12400 training loss = 8.848 validation loss = 8.706093 (minibatch size = 1000)\n",
      "Epoch 30 Step 12500 training loss = 8.849 validation loss = 8.689977 (minibatch size = 1000)\n",
      "Epoch 31 Step 12600 training loss = 8.783 validation loss = 8.702612 (minibatch size = 1000)\n",
      "Epoch 31 Step 12700 training loss = 8.829 validation loss = 8.687289 (minibatch size = 1000)\n",
      "Epoch 31 Step 12800 training loss = 8.783 validation loss = 8.699693 (minibatch size = 1000)\n",
      "Epoch 31 Step 12900 training loss = 8.878 validation loss = 8.687994 (minibatch size = 1000)\n",
      "Epoch 32 Step 13000 training loss = 8.802 validation loss = 8.691289 (minibatch size = 1000)\n",
      "Epoch 32 Step 13100 training loss = 8.813 validation loss = 8.703525 (minibatch size = 1000)\n",
      "Epoch 32 Step 13200 training loss = 8.798 validation loss = 8.686696 (minibatch size = 1000)\n",
      "Epoch 32 Step 13300 training loss = 8.839 validation loss = 8.696392 (minibatch size = 1000)\n",
      "Epoch 33 Step 13400 training loss = 8.805 validation loss = 8.694805 (minibatch size = 1000)\n",
      "Epoch 33 Step 13500 training loss = 8.819 validation loss = 8.690841 (minibatch size = 1000)\n",
      "Epoch 33 Step 13600 training loss = 8.784 validation loss = 8.685231 (minibatch size = 1000)\n",
      "Epoch 33 Step 13700 training loss = 8.843 validation loss = 8.701075 (minibatch size = 1000)\n",
      "Epoch 34 Step 13800 training loss = 8.842 validation loss = 8.681386 (minibatch size = 1000)\n",
      "Epoch 34 Step 13900 training loss = 8.824 validation loss = 8.688038 (minibatch size = 1000)\n",
      "Epoch 34 Step 14000 training loss = 8.761 validation loss = 8.688341 (minibatch size = 1000)\n",
      "Epoch 34 Step 14100 training loss = 8.849 validation loss = 8.678975 (minibatch size = 1000)\n",
      "Epoch 34 Step 14200 training loss = 8.828 validation loss = 8.693830 (minibatch size = 1000)\n",
      "Epoch 35 Step 14300 training loss = 8.826 validation loss = 8.675800 (minibatch size = 1000)\n",
      "Epoch 35 Step 14400 training loss = 8.763 validation loss = 8.696780 (minibatch size = 1000)\n",
      "Epoch 35 Step 14500 training loss = 8.852 validation loss = 8.695410 (minibatch size = 1000)\n",
      "Epoch 35 Step 14600 training loss = 8.817 validation loss = 8.709064 (minibatch size = 1000)\n",
      "Epoch 36 Step 14700 training loss = 8.806 validation loss = 8.685573 (minibatch size = 1000)\n",
      "Epoch 36 Step 14800 training loss = 8.798 validation loss = 8.676903 (minibatch size = 1000)\n",
      "Epoch 36 Step 14900 training loss = 8.815 validation loss = 8.707646 (minibatch size = 1000)\n",
      "Epoch 36 Step 15000 training loss = 8.843 validation loss = 8.674999 (minibatch size = 1000)\n",
      "Epoch 37 Step 15100 training loss = 8.802 validation loss = 8.670593 (minibatch size = 1000)\n",
      "Epoch 37 Step 15200 training loss = 8.803 validation loss = 8.688700 (minibatch size = 1000)\n",
      "Epoch 37 Step 15300 training loss = 8.793 validation loss = 8.683231 (minibatch size = 1000)\n",
      "Epoch 37 Step 15400 training loss = 8.855 validation loss = 8.671584 (minibatch size = 1000)\n",
      "Epoch 38 Step 15500 training loss = 8.800 validation loss = 8.680805 (minibatch size = 1000)\n",
      "Epoch 38 Step 15600 training loss = 8.788 validation loss = 8.687062 (minibatch size = 1000)\n",
      "Epoch 38 Step 15700 training loss = 8.770 validation loss = 8.679642 (minibatch size = 1000)\n",
      "Epoch 38 Step 15800 training loss = 8.849 validation loss = 8.690842 (minibatch size = 1000)\n",
      "Epoch 39 Step 15900 training loss = 8.811 validation loss = 8.689816 (minibatch size = 1000)\n",
      "Epoch 39 Step 16000 training loss = 8.818 validation loss = 8.685704 (minibatch size = 1000)\n",
      "Epoch 39 Step 16100 training loss = 8.742 validation loss = 8.683915 (minibatch size = 1000)\n",
      "Epoch 39 Step 16200 training loss = 8.842 validation loss = 8.689658 (minibatch size = 1000)\n",
      "Epoch 39 Step 16300 training loss = 8.775 validation loss = 8.670333 (minibatch size = 1000)\n",
      "Epoch 40 Step 16400 training loss = 8.836 validation loss = 8.697755 (minibatch size = 1000)\n",
      "Epoch 40 Step 16500 training loss = 8.736 validation loss = 8.699211 (minibatch size = 1000)\n",
      "Epoch 40 Step 16600 training loss = 8.858 validation loss = 8.680321 (minibatch size = 1000)\n",
      "Epoch 40 Step 16700 training loss = 8.803 validation loss = 8.686660 (minibatch size = 1000)\n",
      "Epoch 41 Step 16800 training loss = 8.811 validation loss = 8.679172 (minibatch size = 1000)\n",
      "Epoch 41 Step 16900 training loss = 8.776 validation loss = 8.701338 (minibatch size = 1000)\n",
      "Epoch 41 Step 17000 training loss = 8.826 validation loss = 8.707928 (minibatch size = 1000)\n",
      "Epoch 41 Step 17100 training loss = 8.832 validation loss = 8.689569 (minibatch size = 1000)\n",
      "Epoch 42 Step 17200 training loss = 8.778 validation loss = 8.680861 (minibatch size = 1000)\n",
      "Epoch 42 Step 17300 training loss = 8.820 validation loss = 8.680522 (minibatch size = 1000)\n",
      "Epoch 42 Step 17400 training loss = 8.768 validation loss = 8.688173 (minibatch size = 1000)\n",
      "Epoch 42 Step 17500 training loss = 8.831 validation loss = 8.670934 (minibatch size = 1000)\n",
      "Epoch 43 Step 17600 training loss = 8.780 validation loss = 8.675508 (minibatch size = 1000)\n",
      "Epoch 43 Step 17700 training loss = 8.806 validation loss = 8.696282 (minibatch size = 1000)\n",
      "Epoch 43 Step 17800 training loss = 8.780 validation loss = 8.687908 (minibatch size = 1000)\n",
      "Epoch 43 Step 17900 training loss = 8.848 validation loss = 8.684835 (minibatch size = 1000)\n",
      "Epoch 44 Step 18000 training loss = 8.797 validation loss = 8.685031 (minibatch size = 1000)\n",
      "Epoch 44 Step 18100 training loss = 8.785 validation loss = 8.675506 (minibatch size = 1000)\n",
      "Epoch 44 Step 18200 training loss = 8.766 validation loss = 8.673598 (minibatch size = 1000)\n",
      "Epoch 44 Step 18300 training loss = 8.828 validation loss = 8.696190 (minibatch size = 1000)\n",
      "Epoch 45 Step 18400 training loss = 8.795 validation loss = 8.681201 (minibatch size = 1000)\n",
      "Epoch 45 Step 18500 training loss = 8.793 validation loss = 8.687532 (minibatch size = 1000)\n",
      "Epoch 45 Step 18600 training loss = 8.748 validation loss = 8.685141 (minibatch size = 1000)\n",
      "Epoch 45 Step 18700 training loss = 8.828 validation loss = 8.681365 (minibatch size = 1000)\n",
      "Epoch 45 Step 18800 training loss = 8.787 validation loss = 8.680574 (minibatch size = 1000)\n",
      "Epoch 46 Step 18900 training loss = 8.811 validation loss = 8.667079 (minibatch size = 1000)\n",
      "Epoch 46 Step 19000 training loss = 8.770 validation loss = 8.683226 (minibatch size = 1000)\n",
      "Epoch 46 Step 19100 training loss = 8.830 validation loss = 8.684245 (minibatch size = 1000)\n",
      "Epoch 46 Step 19200 training loss = 8.799 validation loss = 8.689886 (minibatch size = 1000)\n",
      "Epoch 47 Step 19300 training loss = 8.768 validation loss = 8.681862 (minibatch size = 1000)\n",
      "Epoch 47 Step 19400 training loss = 8.787 validation loss = 8.680474 (minibatch size = 1000)\n",
      "Epoch 47 Step 19500 training loss = 8.796 validation loss = 8.695647 (minibatch size = 1000)\n",
      "Epoch 47 Step 19600 training loss = 8.824 validation loss = 8.669428 (minibatch size = 1000)\n",
      "Epoch 48 Step 19700 training loss = 8.764 validation loss = 8.667573 (minibatch size = 1000)\n",
      "Epoch 48 Step 19800 training loss = 8.811 validation loss = 8.684350 (minibatch size = 1000)\n",
      "Epoch 48 Step 19900 training loss = 8.768 validation loss = 8.676745 (minibatch size = 1000)\n",
      "Epoch 48 Step 20000 training loss = 8.821 validation loss = 8.661243 (minibatch size = 1000)\n",
      "Epoch 49 Step 20100 training loss = 8.769 validation loss = 8.664490 (minibatch size = 1000)\n",
      "Epoch 49 Step 20200 training loss = 8.797 validation loss = 8.674020 (minibatch size = 1000)\n",
      "Epoch 49 Step 20300 training loss = 8.748 validation loss = 8.677340 (minibatch size = 1000)\n",
      "Epoch 49 Step 20400 training loss = 8.838 validation loss = 8.688264 (minibatch size = 1000)\n",
      "Epoch 50 Step 20500 training loss = 8.796 validation loss = 8.690716 (minibatch size = 1000)\n",
      "Epoch 50 Step 20600 training loss = 8.817 validation loss = 8.677336 (minibatch size = 1000)\n",
      "Epoch 50 Step 20700 training loss = 8.751 validation loss = 8.675563 (minibatch size = 1000)\n",
      "Epoch 50 Step 20800 training loss = 8.804 validation loss = 8.682063 (minibatch size = 1000)\n",
      "Epoch 50 Step 20900 training loss = 8.775 validation loss = 8.675211 (minibatch size = 344)\n",
      "Epoch 51 Step 21000 training loss = 8.795 validation loss = 8.687518 (minibatch size = 1000)\n",
      "Epoch 51 Step 21100 training loss = 8.723 validation loss = 8.674456 (minibatch size = 1000)\n",
      "Epoch 51 Step 21200 training loss = 8.828 validation loss = 8.668366 (minibatch size = 1000)\n",
      "Epoch 51 Step 21300 training loss = 8.758 validation loss = 8.678570 (minibatch size = 1000)\n",
      "Epoch 52 Step 21400 training loss = 8.798 validation loss = 8.676345 (minibatch size = 1000)\n",
      "Epoch 52 Step 21500 training loss = 8.739 validation loss = 8.692619 (minibatch size = 1000)\n",
      "Epoch 52 Step 21600 training loss = 8.816 validation loss = 8.697020 (minibatch size = 1000)\n",
      "Epoch 52 Step 21700 training loss = 8.803 validation loss = 8.681396 (minibatch size = 1000)\n",
      "Epoch 53 Step 21800 training loss = 8.738 validation loss = 8.674520 (minibatch size = 1000)\n",
      "Epoch 53 Step 21900 training loss = 8.798 validation loss = 8.668702 (minibatch size = 1000)\n",
      "Epoch 53 Step 22000 training loss = 8.752 validation loss = 8.674632 (minibatch size = 1000)\n",
      "Epoch 53 Step 22100 training loss = 8.803 validation loss = 8.661603 (minibatch size = 1000)\n",
      "Epoch 54 Step 22200 training loss = 8.768 validation loss = 8.669821 (minibatch size = 1000)\n",
      "Epoch 54 Step 22300 training loss = 8.782 validation loss = 8.681678 (minibatch size = 1000)\n",
      "Epoch 54 Step 22400 training loss = 8.766 validation loss = 8.682964 (minibatch size = 1000)\n",
      "Epoch 54 Step 22500 training loss = 8.829 validation loss = 8.673794 (minibatch size = 1000)\n",
      "Epoch 55 Step 22600 training loss = 8.756 validation loss = 8.674782 (minibatch size = 1000)\n",
      "Epoch 55 Step 22700 training loss = 8.773 validation loss = 8.665531 (minibatch size = 1000)\n",
      "Epoch 55 Step 22800 training loss = 8.740 validation loss = 8.667217 (minibatch size = 1000)\n",
      "Epoch 55 Step 22900 training loss = 8.827 validation loss = 8.690043 (minibatch size = 1000)\n",
      "Epoch 56 Step 23000 training loss = 8.773 validation loss = 8.674105 (minibatch size = 1000)\n",
      "Epoch 56 Step 23100 training loss = 8.763 validation loss = 8.675063 (minibatch size = 1000)\n",
      "Epoch 56 Step 23200 training loss = 8.737 validation loss = 8.681396 (minibatch size = 1000)\n",
      "Epoch 56 Step 23300 training loss = 8.794 validation loss = 8.675623 (minibatch size = 1000)\n",
      "Epoch 56 Step 23400 training loss = 8.777 validation loss = 8.673004 (minibatch size = 1000)\n",
      "Epoch 57 Step 23500 training loss = 8.803 validation loss = 8.674165 (minibatch size = 1000)\n",
      "Epoch 57 Step 23600 training loss = 8.749 validation loss = 8.682734 (minibatch size = 1000)\n",
      "Epoch 57 Step 23700 training loss = 8.821 validation loss = 8.684043 (minibatch size = 1000)\n",
      "Epoch 57 Step 23800 training loss = 8.789 validation loss = 8.684231 (minibatch size = 1000)\n",
      "Epoch 58 Step 23900 training loss = 8.780 validation loss = 8.669815 (minibatch size = 1000)\n",
      "Epoch 58 Step 24000 training loss = 8.755 validation loss = 8.673755 (minibatch size = 1000)\n",
      "Epoch 58 Step 24100 training loss = 8.800 validation loss = 8.690735 (minibatch size = 1000)\n",
      "Epoch 58 Step 24200 training loss = 8.799 validation loss = 8.677299 (minibatch size = 1000)\n",
      "Epoch 59 Step 24300 training loss = 8.747 validation loss = 8.667454 (minibatch size = 1000)\n",
      "Epoch 59 Step 24400 training loss = 8.780 validation loss = 8.680703 (minibatch size = 1000)\n",
      "Epoch 59 Step 24500 training loss = 8.741 validation loss = 8.674384 (minibatch size = 1000)\n",
      "Epoch 59 Step 24600 training loss = 8.835 validation loss = 8.663921 (minibatch size = 1000)\n",
      "Epoch 60 Step 24700 training loss = 8.755 validation loss = 8.677269 (minibatch size = 1000)\n",
      "Epoch 60 Step 24800 training loss = 8.759 validation loss = 8.684771 (minibatch size = 1000)\n",
      "Epoch 60 Step 24900 training loss = 8.739 validation loss = 8.671602 (minibatch size = 1000)\n",
      "Epoch 60 Step 25000 training loss = 8.825 validation loss = 8.680058 (minibatch size = 1000)\n",
      "Early stopping!\n",
      "==================================================validation result==================================================\n",
      "the best step is 20000 with minimum validation error = 8.661243438720703\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H = 90\n",
    "z = 0.5\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "Q7_model = myMLP_2()\n",
    "net = Q7_model.Net(input_shape, device, H, dropout = True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "model = Q7_model.train(device, net, nepoch, log_interval, optimizer, z, subtrainloader, validloader, verbose = True, customized_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 279811,
     "status": "ok",
     "timestamp": 1608736104148,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "xFZsZQUNtWzo",
    "outputId": "12adf4f2-4d7b-413c-e779-b6dc17f49c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 90, z = 0.5, optimizer = Adam, with dropout layers and loss function = L1 + L2,test_RMSE = 8.868124172787063\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_RMSE = Q7_model.test(device, H, model, testloader)\n",
    "print('when H = {}, z = {}, optimizer = Adam, with dropout layers and loss function = L1 + L2,test_RMSE = {}'.format(H, z, test_RMSE))\n",
    "Q7_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1617456,
     "status": "ok",
     "timestamp": 1608737443249,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "hNGk0f5ETTSi",
    "outputId": "a38de975-99d3-4c59-f57a-c4460ac4b65e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when z = 0.0, test RMSE = 9.093717888731122\n",
      "when z = 0.1, test RMSE = 8.856340076544237\n",
      "when z = 0.5, test RMSE = 8.875307871596679\n",
      "when z = 0.9, test RMSE = 8.866222049150258\n",
      "when z = 1.0, test RMSE = 8.87368043172306\n"
     ]
    }
   ],
   "source": [
    "# GPU device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Settings\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "lr = 0.001\n",
    "H = 90\n",
    "z_list = [0.0, 0.1, 0.5, 0.9, 1.0]\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "for z in z_list:\n",
    "    Q7_model = myMLP_2()\n",
    "    net = Q7_model.Net(input_shape, device, H, dropout = True)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    model = Q7_model.train(device, net, nepoch, log_interval, optimizer, z, subtrainloader, validloader, verbose = False, customized_loss = False)\n",
    "    test_RMSE = Q7_model.test(device, H, model, testloader)\n",
    "    print('when z = {}, test RMSE = {}'.format(z, test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從結果可以發現，完全只依賴L2 loss(z=1)或完全只依賴L1 loss(z=0)來更新參數對模型並不是最好的選擇；經過調整參數後可以觀察到z=0.1時模型的結果會最好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNdGTRFDfVQn"
   },
   "source": [
    "### Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1846581,
     "status": "ok",
     "timestamp": 1608737673809,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "ItwzkwaEfAc-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "14017234-1c80-4462-d2f0-ca28e2668246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 100 training loss = 10.991 validation loss = 10.685866 (minibatch size = 1000)\n",
      "Epoch 1 Step 200 training loss = 10.523 validation loss = 10.159313 (minibatch size = 1000)\n",
      "Epoch 1 Step 300 training loss = 9.961 validation loss = 9.471992 (minibatch size = 1000)\n",
      "Epoch 1 Step 400 training loss = 9.626 validation loss = 9.387156 (minibatch size = 1000)\n",
      "Epoch 2 Step 500 training loss = 9.627 validation loss = 9.296260 (minibatch size = 1000)\n",
      "Epoch 2 Step 600 training loss = 9.503 validation loss = 9.263186 (minibatch size = 1000)\n",
      "Epoch 2 Step 700 training loss = 9.528 validation loss = 9.255858 (minibatch size = 1000)\n",
      "Epoch 2 Step 800 training loss = 9.484 validation loss = 9.286282 (minibatch size = 1000)\n",
      "Epoch 3 Step 900 training loss = 9.429 validation loss = 9.203110 (minibatch size = 1000)\n",
      "Epoch 3 Step 1000 training loss = 9.460 validation loss = 9.200662 (minibatch size = 1000)\n",
      "Epoch 3 Step 1100 training loss = 9.385 validation loss = 9.139770 (minibatch size = 1000)\n",
      "Epoch 3 Step 1200 training loss = 9.430 validation loss = 9.118522 (minibatch size = 1000)\n",
      "Epoch 4 Step 1300 training loss = 9.373 validation loss = 9.118357 (minibatch size = 1000)\n",
      "Epoch 4 Step 1400 training loss = 9.391 validation loss = 9.135944 (minibatch size = 1000)\n",
      "Epoch 4 Step 1500 training loss = 9.370 validation loss = 9.074180 (minibatch size = 1000)\n",
      "Epoch 4 Step 1600 training loss = 9.401 validation loss = 9.112256 (minibatch size = 1000)\n",
      "Epoch 5 Step 1700 training loss = 9.326 validation loss = 9.071866 (minibatch size = 1000)\n",
      "Epoch 5 Step 1800 training loss = 9.378 validation loss = 9.105335 (minibatch size = 1000)\n",
      "Epoch 5 Step 1900 training loss = 9.325 validation loss = 9.067001 (minibatch size = 1000)\n",
      "Epoch 5 Step 2000 training loss = 9.353 validation loss = 9.058674 (minibatch size = 1000)\n",
      "Epoch 6 Step 2100 training loss = 9.338 validation loss = 9.134729 (minibatch size = 1000)\n",
      "Epoch 6 Step 2200 training loss = 9.350 validation loss = 9.037001 (minibatch size = 1000)\n",
      "Epoch 6 Step 2300 training loss = 9.278 validation loss = 9.087163 (minibatch size = 1000)\n",
      "Epoch 6 Step 2400 training loss = 9.313 validation loss = 9.112457 (minibatch size = 1000)\n",
      "Epoch 6 Step 2500 training loss = 9.295 validation loss = 9.042386 (minibatch size = 1000)\n",
      "Epoch 7 Step 2600 training loss = 9.351 validation loss = 9.072527 (minibatch size = 1000)\n",
      "Epoch 7 Step 2700 training loss = 9.262 validation loss = 9.009396 (minibatch size = 1000)\n",
      "Epoch 7 Step 2800 training loss = 9.320 validation loss = 9.047411 (minibatch size = 1000)\n",
      "Epoch 7 Step 2900 training loss = 9.280 validation loss = 9.062748 (minibatch size = 1000)\n",
      "Epoch 8 Step 3000 training loss = 9.301 validation loss = 9.011084 (minibatch size = 1000)\n",
      "Epoch 8 Step 3100 training loss = 9.260 validation loss = 9.045547 (minibatch size = 1000)\n",
      "Epoch 8 Step 3200 training loss = 9.287 validation loss = 9.087555 (minibatch size = 1000)\n",
      "Epoch 8 Step 3300 training loss = 9.331 validation loss = 9.022737 (minibatch size = 1000)\n",
      "Epoch 9 Step 3400 training loss = 9.245 validation loss = 9.015829 (minibatch size = 1000)\n",
      "Epoch 9 Step 3500 training loss = 9.289 validation loss = 8.969239 (minibatch size = 1000)\n",
      "Epoch 9 Step 3600 training loss = 9.231 validation loss = 8.989163 (minibatch size = 1000)\n",
      "Epoch 9 Step 3700 training loss = 9.312 validation loss = 9.016470 (minibatch size = 1000)\n",
      "Epoch 10 Step 3800 training loss = 9.238 validation loss = 9.006598 (minibatch size = 1000)\n",
      "Epoch 10 Step 3900 training loss = 9.246 validation loss = 9.022388 (minibatch size = 1000)\n",
      "Epoch 10 Step 4000 training loss = 9.225 validation loss = 9.002366 (minibatch size = 1000)\n",
      "Epoch 10 Step 4100 training loss = 9.305 validation loss = 9.023516 (minibatch size = 1000)\n",
      "Epoch 11 Step 4200 training loss = 9.259 validation loss = 9.002333 (minibatch size = 1000)\n",
      "Epoch 11 Step 4300 training loss = 9.249 validation loss = 8.967321 (minibatch size = 1000)\n",
      "Epoch 11 Step 4400 training loss = 9.215 validation loss = 8.981770 (minibatch size = 1000)\n",
      "Epoch 11 Step 4500 training loss = 9.278 validation loss = 9.040580 (minibatch size = 1000)\n",
      "Epoch 12 Step 4600 training loss = 9.234 validation loss = 8.991197 (minibatch size = 1000)\n",
      "Epoch 12 Step 4700 training loss = 9.248 validation loss = 9.030098 (minibatch size = 1000)\n",
      "Epoch 12 Step 4800 training loss = 9.183 validation loss = 8.971239 (minibatch size = 1000)\n",
      "Epoch 12 Step 4900 training loss = 9.267 validation loss = 8.967682 (minibatch size = 1000)\n",
      "Epoch 12 Step 5000 training loss = 9.217 validation loss = 8.977653 (minibatch size = 1000)\n",
      "Epoch 13 Step 5100 training loss = 9.266 validation loss = 8.993255 (minibatch size = 1000)\n",
      "Epoch 13 Step 5200 training loss = 9.189 validation loss = 8.993255 (minibatch size = 1000)\n",
      "Epoch 13 Step 5300 training loss = 9.263 validation loss = 8.994327 (minibatch size = 1000)\n",
      "Epoch 13 Step 5400 training loss = 9.243 validation loss = 9.042758 (minibatch size = 1000)\n",
      "Epoch 14 Step 5500 training loss = 9.208 validation loss = 8.983682 (minibatch size = 1000)\n",
      "Epoch 14 Step 5600 training loss = 9.219 validation loss = 8.981132 (minibatch size = 1000)\n",
      "Epoch 14 Step 5700 training loss = 9.222 validation loss = 8.969141 (minibatch size = 1000)\n",
      "Epoch 14 Step 5800 training loss = 9.231 validation loss = 8.968793 (minibatch size = 1000)\n",
      "Epoch 15 Step 5900 training loss = 9.230 validation loss = 8.974913 (minibatch size = 1000)\n",
      "Epoch 15 Step 6000 training loss = 9.241 validation loss = 8.992884 (minibatch size = 1000)\n",
      "Epoch 15 Step 6100 training loss = 9.184 validation loss = 8.945076 (minibatch size = 1000)\n",
      "Epoch 15 Step 6200 training loss = 9.271 validation loss = 8.981615 (minibatch size = 1000)\n",
      "Epoch 16 Step 6300 training loss = 9.196 validation loss = 8.997720 (minibatch size = 1000)\n",
      "Epoch 16 Step 6400 training loss = 9.223 validation loss = 8.973085 (minibatch size = 1000)\n",
      "Epoch 16 Step 6500 training loss = 9.210 validation loss = 8.958840 (minibatch size = 1000)\n",
      "Epoch 16 Step 6600 training loss = 9.243 validation loss = 8.984498 (minibatch size = 1000)\n",
      "Epoch 17 Step 6700 training loss = 9.235 validation loss = 9.016246 (minibatch size = 1000)\n",
      "Epoch 17 Step 6800 training loss = 9.227 validation loss = 8.949169 (minibatch size = 1000)\n",
      "Epoch 17 Step 6900 training loss = 9.177 validation loss = 8.973186 (minibatch size = 1000)\n",
      "Epoch 17 Step 7000 training loss = 9.243 validation loss = 9.031734 (minibatch size = 1000)\n",
      "Epoch 17 Step 7100 training loss = 9.200 validation loss = 8.956811 (minibatch size = 1000)\n",
      "Epoch 18 Step 7200 training loss = 9.228 validation loss = 8.951707 (minibatch size = 1000)\n",
      "Epoch 18 Step 7300 training loss = 9.167 validation loss = 8.979540 (minibatch size = 1000)\n",
      "Epoch 18 Step 7400 training loss = 9.237 validation loss = 8.978733 (minibatch size = 1000)\n",
      "Epoch 18 Step 7500 training loss = 9.201 validation loss = 9.000911 (minibatch size = 1000)\n",
      "Epoch 19 Step 7600 training loss = 9.221 validation loss = 8.939469 (minibatch size = 1000)\n",
      "Epoch 19 Step 7700 training loss = 9.187 validation loss = 8.971126 (minibatch size = 1000)\n",
      "Epoch 19 Step 7800 training loss = 9.214 validation loss = 8.991446 (minibatch size = 1000)\n",
      "Epoch 19 Step 7900 training loss = 9.215 validation loss = 8.964972 (minibatch size = 1000)\n",
      "Epoch 20 Step 8000 training loss = 9.167 validation loss = 8.956303 (minibatch size = 1000)\n",
      "Epoch 20 Step 8100 training loss = 9.222 validation loss = 8.951882 (minibatch size = 1000)\n",
      "Epoch 20 Step 8200 training loss = 9.148 validation loss = 8.929281 (minibatch size = 1000)\n",
      "Epoch 20 Step 8300 training loss = 9.247 validation loss = 8.944707 (minibatch size = 1000)\n",
      "Epoch 21 Step 8400 training loss = 9.191 validation loss = 8.970902 (minibatch size = 1000)\n",
      "Epoch 21 Step 8500 training loss = 9.226 validation loss = 8.995130 (minibatch size = 1000)\n",
      "Epoch 21 Step 8600 training loss = 9.154 validation loss = 8.947758 (minibatch size = 1000)\n",
      "Epoch 21 Step 8700 training loss = 9.231 validation loss = 8.989198 (minibatch size = 1000)\n",
      "Epoch 22 Step 8800 training loss = 9.224 validation loss = 8.992678 (minibatch size = 1000)\n",
      "Epoch 22 Step 8900 training loss = 9.227 validation loss = 8.948642 (minibatch size = 1000)\n",
      "Epoch 22 Step 9000 training loss = 9.161 validation loss = 8.960723 (minibatch size = 1000)\n",
      "Epoch 22 Step 9100 training loss = 9.254 validation loss = 8.988461 (minibatch size = 1000)\n",
      "Epoch 23 Step 9200 training loss = 9.204 validation loss = 9.006571 (minibatch size = 1000)\n",
      "Epoch 23 Step 9300 training loss = 9.219 validation loss = 8.987291 (minibatch size = 1000)\n",
      "Epoch 23 Step 9400 training loss = 9.140 validation loss = 8.944161 (minibatch size = 1000)\n",
      "Epoch 23 Step 9500 training loss = 9.220 validation loss = 8.924382 (minibatch size = 1000)\n",
      "Epoch 23 Step 9600 training loss = 9.187 validation loss = 8.958922 (minibatch size = 1000)\n",
      "Epoch 24 Step 9700 training loss = 9.218 validation loss = 8.948457 (minibatch size = 1000)\n",
      "Epoch 24 Step 9800 training loss = 9.152 validation loss = 8.946554 (minibatch size = 1000)\n",
      "Epoch 24 Step 9900 training loss = 9.213 validation loss = 8.969571 (minibatch size = 1000)\n",
      "Epoch 24 Step 10000 training loss = 9.192 validation loss = 8.989465 (minibatch size = 1000)\n",
      "Epoch 25 Step 10100 training loss = 9.165 validation loss = 8.986666 (minibatch size = 1000)\n",
      "Epoch 25 Step 10200 training loss = 9.211 validation loss = 8.978174 (minibatch size = 1000)\n",
      "Epoch 25 Step 10300 training loss = 9.160 validation loss = 8.943827 (minibatch size = 1000)\n",
      "Epoch 25 Step 10400 training loss = 9.209 validation loss = 8.941093 (minibatch size = 1000)\n",
      "Epoch 26 Step 10500 training loss = 9.176 validation loss = 8.962065 (minibatch size = 1000)\n",
      "Epoch 26 Step 10600 training loss = 9.196 validation loss = 8.934161 (minibatch size = 1000)\n",
      "Epoch 26 Step 10700 training loss = 9.152 validation loss = 8.914799 (minibatch size = 1000)\n",
      "Epoch 26 Step 10800 training loss = 9.226 validation loss = 8.947151 (minibatch size = 1000)\n",
      "Epoch 27 Step 10900 training loss = 9.181 validation loss = 8.965943 (minibatch size = 1000)\n",
      "Epoch 27 Step 11000 training loss = 9.177 validation loss = 8.961926 (minibatch size = 1000)\n",
      "Epoch 27 Step 11100 training loss = 9.139 validation loss = 8.948411 (minibatch size = 1000)\n",
      "Epoch 27 Step 11200 training loss = 9.205 validation loss = 8.955065 (minibatch size = 1000)\n",
      "Epoch 28 Step 11300 training loss = 9.187 validation loss = 8.978913 (minibatch size = 1000)\n",
      "Epoch 28 Step 11400 training loss = 9.209 validation loss = 8.933876 (minibatch size = 1000)\n",
      "Epoch 28 Step 11500 training loss = 9.125 validation loss = 8.933084 (minibatch size = 1000)\n",
      "Epoch 28 Step 11600 training loss = 9.188 validation loss = 8.978881 (minibatch size = 1000)\n",
      "Epoch 28 Step 11700 training loss = 9.176 validation loss = 8.937217 (minibatch size = 1000)\n",
      "Epoch 29 Step 11800 training loss = 9.199 validation loss = 8.951900 (minibatch size = 1000)\n",
      "Epoch 29 Step 11900 training loss = 9.125 validation loss = 8.961362 (minibatch size = 1000)\n",
      "Epoch 29 Step 12000 training loss = 9.222 validation loss = 8.937232 (minibatch size = 1000)\n",
      "Epoch 29 Step 12100 training loss = 9.175 validation loss = 8.967251 (minibatch size = 1000)\n",
      "Epoch 30 Step 12200 training loss = 9.201 validation loss = 8.916473 (minibatch size = 1000)\n",
      "Epoch 30 Step 12300 training loss = 9.144 validation loss = 8.982722 (minibatch size = 1000)\n",
      "Epoch 30 Step 12400 training loss = 9.189 validation loss = 8.949260 (minibatch size = 1000)\n",
      "Epoch 30 Step 12500 training loss = 9.197 validation loss = 8.958841 (minibatch size = 1000)\n",
      "Epoch 31 Step 12600 training loss = 9.130 validation loss = 8.979635 (minibatch size = 1000)\n",
      "Epoch 31 Step 12700 training loss = 9.215 validation loss = 8.933156 (minibatch size = 1000)\n",
      "Epoch 31 Step 12800 training loss = 9.126 validation loss = 8.926890 (minibatch size = 1000)\n",
      "Epoch 31 Step 12900 training loss = 9.207 validation loss = 8.960906 (minibatch size = 1000)\n",
      "Epoch 32 Step 13000 training loss = 9.159 validation loss = 8.955810 (minibatch size = 1000)\n",
      "Epoch 32 Step 13100 training loss = 9.180 validation loss = 8.994241 (minibatch size = 1000)\n",
      "Epoch 32 Step 13200 training loss = 9.125 validation loss = 8.927900 (minibatch size = 1000)\n",
      "Epoch 32 Step 13300 training loss = 9.212 validation loss = 8.987102 (minibatch size = 1000)\n",
      "Epoch 33 Step 13400 training loss = 9.159 validation loss = 8.946158 (minibatch size = 1000)\n",
      "Epoch 33 Step 13500 training loss = 9.188 validation loss = 8.929312 (minibatch size = 1000)\n",
      "Epoch 33 Step 13600 training loss = 9.137 validation loss = 8.929249 (minibatch size = 1000)\n",
      "Epoch 33 Step 13700 training loss = 9.182 validation loss = 8.964822 (minibatch size = 1000)\n",
      "Epoch 34 Step 13800 training loss = 9.204 validation loss = 8.956855 (minibatch size = 1000)\n",
      "Epoch 34 Step 13900 training loss = 9.175 validation loss = 8.983364 (minibatch size = 1000)\n",
      "Epoch 34 Step 14000 training loss = 9.131 validation loss = 8.933349 (minibatch size = 1000)\n",
      "Epoch 34 Step 14100 training loss = 9.189 validation loss = 8.919235 (minibatch size = 1000)\n",
      "Epoch 34 Step 14200 training loss = 9.150 validation loss = 8.936979 (minibatch size = 1000)\n",
      "Epoch 35 Step 14300 training loss = 9.210 validation loss = 8.923801 (minibatch size = 1000)\n",
      "Epoch 35 Step 14400 training loss = 9.137 validation loss = 8.956296 (minibatch size = 1000)\n",
      "Epoch 35 Step 14500 training loss = 9.203 validation loss = 8.974983 (minibatch size = 1000)\n",
      "Epoch 35 Step 14600 training loss = 9.160 validation loss = 8.996873 (minibatch size = 1000)\n",
      "Epoch 36 Step 14700 training loss = 9.165 validation loss = 8.956511 (minibatch size = 1000)\n",
      "Epoch 36 Step 14800 training loss = 9.179 validation loss = 8.972127 (minibatch size = 1000)\n",
      "Epoch 36 Step 14900 training loss = 9.157 validation loss = 8.947620 (minibatch size = 1000)\n",
      "Epoch 36 Step 15000 training loss = 9.214 validation loss = 8.957154 (minibatch size = 1000)\n",
      "Epoch 37 Step 15100 training loss = 9.152 validation loss = 8.945541 (minibatch size = 1000)\n",
      "Epoch 37 Step 15200 training loss = 9.175 validation loss = 8.959012 (minibatch size = 1000)\n",
      "Epoch 37 Step 15300 training loss = 9.145 validation loss = 8.912577 (minibatch size = 1000)\n",
      "Epoch 37 Step 15400 training loss = 9.212 validation loss = 8.945266 (minibatch size = 1000)\n",
      "Epoch 38 Step 15500 training loss = 9.168 validation loss = 8.960579 (minibatch size = 1000)\n",
      "Epoch 38 Step 15600 training loss = 9.180 validation loss = 8.952505 (minibatch size = 1000)\n",
      "Epoch 38 Step 15700 training loss = 9.121 validation loss = 8.928529 (minibatch size = 1000)\n",
      "Epoch 38 Step 15800 training loss = 9.199 validation loss = 8.984049 (minibatch size = 1000)\n",
      "Epoch 39 Step 15900 training loss = 9.194 validation loss = 8.986173 (minibatch size = 1000)\n",
      "Epoch 39 Step 16000 training loss = 9.204 validation loss = 8.957939 (minibatch size = 1000)\n",
      "Epoch 39 Step 16100 training loss = 9.090 validation loss = 8.952837 (minibatch size = 1000)\n",
      "Epoch 39 Step 16200 training loss = 9.186 validation loss = 8.981520 (minibatch size = 1000)\n",
      "Epoch 39 Step 16300 training loss = 9.157 validation loss = 8.934738 (minibatch size = 1000)\n",
      "Epoch 40 Step 16400 training loss = 9.196 validation loss = 8.960052 (minibatch size = 1000)\n",
      "Epoch 40 Step 16500 training loss = 9.110 validation loss = 8.947687 (minibatch size = 1000)\n",
      "Epoch 40 Step 16600 training loss = 9.226 validation loss = 8.936352 (minibatch size = 1000)\n",
      "Epoch 40 Step 16700 training loss = 9.139 validation loss = 8.934675 (minibatch size = 1000)\n",
      "Epoch 41 Step 16800 training loss = 9.190 validation loss = 8.945214 (minibatch size = 1000)\n",
      "Epoch 41 Step 16900 training loss = 9.150 validation loss = 8.965662 (minibatch size = 1000)\n",
      "Epoch 41 Step 17000 training loss = 9.175 validation loss = 8.958927 (minibatch size = 1000)\n",
      "Epoch 41 Step 17100 training loss = 9.191 validation loss = 8.976942 (minibatch size = 1000)\n",
      "Epoch 42 Step 17200 training loss = 9.134 validation loss = 8.973145 (minibatch size = 1000)\n",
      "Epoch 42 Step 17300 training loss = 9.193 validation loss = 8.948993 (minibatch size = 1000)\n",
      "Epoch 42 Step 17400 training loss = 9.116 validation loss = 8.943868 (minibatch size = 1000)\n",
      "Epoch 42 Step 17500 training loss = 9.191 validation loss = 8.931787 (minibatch size = 1000)\n",
      "Epoch 43 Step 17600 training loss = 9.155 validation loss = 8.955571 (minibatch size = 1000)\n",
      "Epoch 43 Step 17700 training loss = 9.156 validation loss = 8.968488 (minibatch size = 1000)\n",
      "Epoch 43 Step 17800 training loss = 9.131 validation loss = 8.935441 (minibatch size = 1000)\n",
      "Epoch 43 Step 17900 training loss = 9.199 validation loss = 8.969753 (minibatch size = 1000)\n",
      "Epoch 44 Step 18000 training loss = 9.152 validation loss = 8.972370 (minibatch size = 1000)\n",
      "Epoch 44 Step 18100 training loss = 9.177 validation loss = 8.953186 (minibatch size = 1000)\n",
      "Epoch 44 Step 18200 training loss = 9.109 validation loss = 8.967823 (minibatch size = 1000)\n",
      "Epoch 44 Step 18300 training loss = 9.189 validation loss = 8.955284 (minibatch size = 1000)\n",
      "Epoch 45 Step 18400 training loss = 9.166 validation loss = 8.960918 (minibatch size = 1000)\n",
      "Epoch 45 Step 18500 training loss = 9.162 validation loss = 8.949769 (minibatch size = 1000)\n",
      "Epoch 45 Step 18600 training loss = 9.102 validation loss = 8.944313 (minibatch size = 1000)\n",
      "Epoch 45 Step 18700 training loss = 9.168 validation loss = 8.976415 (minibatch size = 1000)\n",
      "Epoch 45 Step 18800 training loss = 9.157 validation loss = 8.963943 (minibatch size = 1000)\n",
      "Epoch 46 Step 18900 training loss = 9.200 validation loss = 8.970259 (minibatch size = 1000)\n",
      "Epoch 46 Step 19000 training loss = 9.107 validation loss = 8.962115 (minibatch size = 1000)\n",
      "Epoch 46 Step 19100 training loss = 9.183 validation loss = 8.976747 (minibatch size = 1000)\n",
      "Epoch 46 Step 19200 training loss = 9.174 validation loss = 8.991666 (minibatch size = 1000)\n",
      "Epoch 47 Step 19300 training loss = 9.142 validation loss = 8.960649 (minibatch size = 1000)\n",
      "Epoch 47 Step 19400 training loss = 9.146 validation loss = 8.955218 (minibatch size = 1000)\n",
      "Epoch 47 Step 19500 training loss = 9.160 validation loss = 8.975481 (minibatch size = 1000)\n",
      "Epoch 47 Step 19600 training loss = 9.195 validation loss = 8.950771 (minibatch size = 1000)\n",
      "Epoch 48 Step 19700 training loss = 9.132 validation loss = 8.940872 (minibatch size = 1000)\n",
      "Epoch 48 Step 19800 training loss = 9.187 validation loss = 8.946631 (minibatch size = 1000)\n",
      "Epoch 48 Step 19900 training loss = 9.138 validation loss = 8.943299 (minibatch size = 1000)\n",
      "Epoch 48 Step 20000 training loss = 9.192 validation loss = 8.951224 (minibatch size = 1000)\n",
      "Epoch 49 Step 20100 training loss = 9.138 validation loss = 8.966769 (minibatch size = 1000)\n",
      "Epoch 49 Step 20200 training loss = 9.172 validation loss = 8.944441 (minibatch size = 1000)\n",
      "Epoch 49 Step 20300 training loss = 9.117 validation loss = 8.942099 (minibatch size = 1000)\n",
      "Early stopping!\n",
      "==================================================validation result==================================================\n",
      "the best step is 15300 with minimum validation error = 8.912576675415039\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nepoch = 100\n",
    "log_interval = 100\n",
    "H = 90\n",
    "z = 0\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "Q8_model = myMLP_2()\n",
    "net = Q8_model.Net(input_shape, device, H, dropout = True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "model = Q8_model.train(device, net, nepoch, log_interval, optimizer, z, subtrainloader, validloader, verbose = True, customized_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 1845101,
     "status": "ok",
     "timestamp": 1608737674180,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "X-krWhUwfHUT",
    "outputId": "079c043d-6b1d-4d76-df13-abc3b1f10dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when H = 90, z = 0, optimizer = Adam, with dropout layers and loss function = qloss, test_RMSE = 9.059434352881812\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_RMSE = Q8_model.test(device, H, model, testloader)\n",
    "print('when H = {}, z = {}, optimizer = Adam, with dropout layers and loss function = qloss, test_RMSE = {}'.format(H, z, test_RMSE))\n",
    "Q8_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3424095,
     "status": "ok",
     "timestamp": 1608739255678,
     "user": {
      "displayName": "林聖硯",
      "photoUrl": "",
      "userId": "01728102923423697608"
     },
     "user_tz": -480
    },
    "id": "4lzcNV6whxnY",
    "outputId": "54dfbb90-e2ff-4a21-f849-b3a9078295e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when z = 0.0, test RMSE = 9.08860612044098\n",
      "when z = 0.1, test RMSE = 8.87004020718403\n",
      "when z = 0.5, test RMSE = 8.85698223468173\n",
      "when z = 0.9, test RMSE = 8.860057186242193\n",
      "when z = 1.0, test RMSE = 8.877733771913698\n"
     ]
    }
   ],
   "source": [
    "# GPU device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Settings\n",
    "nepoch = 100\n",
    "H = 90\n",
    "log_interval = 100\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "\n",
    "z_list = [0.0, 0.1, 0.5, 0.9, 1.0]\n",
    "\n",
    "for z in z_list:\n",
    "    Q8_model = myMLP_2()\n",
    "    net = Q8_model.Net(input_shape, device, H, dropout = True)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    model = Q8_model.train(device, net, nepoch, log_interval, optimizer, z, subtrainloader, validloader, verbose = False, customized_loss = True)\n",
    "    test_RMSE = Q8_model.test(device, H, model, testloader)\n",
    "    print('when z = {}, test RMSE = {}'.format(z, test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "與前一提的結果類似，完全只依賴L2 loss(z=1)或完全只依賴qloss(z=0)來更新參數對模型並不是最好的選擇；經過調整參數後可以觀察到z=0.5，也就是各使用一半的loss來更新模型時結果會最好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
